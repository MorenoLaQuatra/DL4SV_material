
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Convolutional Neural Networks &#8212; Deep Learning for Speech and Vision</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js?v=afe5de03"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '2_cnns';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Transformers" href="3_transformers.html" />
    <link rel="prev" title="Introduction to Deep Learning" href="1_dl_intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/cover_2.png" class="logo__image only-light" alt="Deep Learning for Speech and Vision - Home"/>
    <script>document.write(`<img src="_static/cover_2.png" class="logo__image only-dark" alt="Deep Learning for Speech and Vision - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Deep Learning for Speech and Vision
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_dl_intro.html">Introduction to Deep Learning</a></li>


<li class="toctree-l1 current active"><a class="current reference internal" href="#">Convolutional Neural Networks</a></li>




<li class="toctree-l1"><a class="reference internal" href="3_transformers.html">Transformers</a></li>



<li class="toctree-l1"><a class="reference internal" href="4_libraries.html">Deep Learning Libraries</a></li>


<li class="toctree-l1"><a class="reference internal" href="5_applications.html">Applications and final projects</a></li>



<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F2_cnns.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/2_cnns.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Convolutional Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Convolutional Neural Networks</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-image-data">Properties of Image Data</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-network-architecture">Convolutional Neural Network Architecture</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-components">Basic Components</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-layers">Convolutional Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-layers">Pooling Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stride-and-padding">Stride and Padding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#receptive-field">Receptive Field</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-cnn-architectures">Common CNN Architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lenet-5">LeNet-5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet">AlexNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet">ResNet</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convnets-for-audio-data">ConvNets for Audio Data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-convolutional-layers">1-D Convolutional Layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-layers-for-temporal-patterns">Convolutional Layers for Temporal Patterns</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectrogram-representation">Spectrogram Representation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#short-time-fourier-transform">Short-Time Fourier Transform</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mel-spectrogram">Mel-Spectrogram</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#audio-cnn-architectures">Audio CNN Architectures</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-a-cnn-with-pytorch">Implementing a CNN with PyTorch</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">MNIST</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#italian-intent-classification-italic">ITALian Intent Classification (ITALIC)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataloader">DataLoader</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Convolutional Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizers-and-schedulers">Optimizers and Schedulers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-my-first-cnn">Training My (First) CNN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment">Assignment</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="convolutional-neural-networks">
<h1>Convolutional Neural Networks<a class="headerlink" href="#convolutional-neural-networks" title="Link to this heading">#</a></h1>
<figure class="align-default" id="cover">
<a class="reference internal image-reference" href="_images/cover_cnn.png"><img alt="cover" src="_images/cover_cnn.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Image generated using <a class="reference external" href="https://huggingface.co/spaces/mrfakename/OpenDalleV1.1-GPU-Demo">OpenDALL-E</a></span><a class="headerlink" href="#cover" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h1>
<p>Convolutional Neural Networks (CNNs) are a class of deep neural networks that are widely used in computer vision applications. CNNs are designed to process data that have a grid-like or lattice-like topology, such as images, speech signals, and text.</p>
<p>The convolutional neural network architecture was first introduced in the 1980s by Yann LeCun and colleagues <span id="id1">[<a class="reference internal" href="references.html#id5" title="Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.">LBD+89</a>]</span>. The first CNN was designed to recognize handwritten digits as the one shown in Figure <a class="reference internal" href="#mnist"><span class="std std-ref">The MNIST dataset</span></a>
. The network was trained on the MNIST dataset, a collection of 60000 handwritten digits.</p>
<figure class="align-default" id="mnist">
<a class="reference internal image-reference" href="_images/mnist.png"><img alt="mnist" src="_images/mnist.png" style="width: 60%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">The MNIST dataset</span><a class="headerlink" href="#mnist" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="properties-of-image-data">
<h2>Properties of Image Data<a class="headerlink" href="#properties-of-image-data" title="Link to this heading">#</a></h2>
<p>Images are a special type of data that are characterized by three properties:</p>
<ul class="simple">
<li><p><strong>High dimensionality</strong>: images are represented as a matrix of pixels, where each pixel is a value between 0 and 255. For example, a 224x224 RGB image is represented as a 224x224x3 matrix (<span class="math notranslate nohighlight">\(224 \times 224 \times 3 = 150528\)</span>).</p></li>
<li><p><strong>Spatial correlation</strong>: neaby pixel in an image are correlated. For example, in a picture of a cat, the pixels that represent the cat’s fur are likely to be similar.</p></li>
<li><p><strong>Invariance to geometric transformations</strong>: the content of an image is invariant to geometric transformations such as translation, rotation, and scaling. For example, a picture of a cat is still a picture of a cat if we rotate it by 90 degrees.</p></li>
</ul>
<p>Those properties are directly related to the fact that is really difficult to use a fully connected neural network to process images.</p>
<ul class="simple">
<li><p>The high dimensionality of images makes the training of a fully connected neural network infeasible. Even a shallow network receiving as input a 224x224 RGB image would have 150528 input units in the first layer. This number would increase exponentially with the number of layers (2 layers <span class="math notranslate nohighlight">\(150528^2\)</span>, 3 layers <span class="math notranslate nohighlight">\(150528^3\)</span>, etc.).</p></li>
<li><p>The spatial correlation of images is not exploited by fully connected neural networks. In a fully connected neural network, each input unit is connected to each output unit. This means that the network would learn a different weight for each pixel in the image, regardless of its position. This is not desirable because the network would not be able to learn the spatial correlation between pixels.</p></li>
<li><p>Similarly, a fully connected neural network would not be able to learn the invariance to geometric transformations. If we translate, rotate, or scale an image, the network sees a completely different input.</p></li>
</ul>
<p>Convolutional Neural Networks are designed to overcome these limitations.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="convolutional-neural-network-architecture">
<h1>Convolutional Neural Network Architecture<a class="headerlink" href="#convolutional-neural-network-architecture" title="Link to this heading">#</a></h1>
<p>The architecture of a convolutional neural network is composed of three main components:</p>
<ul class="simple">
<li><p><strong>Convolutional layers</strong>: these layers are responsible for extracting features from the input data. A convolutional layer is composed of a set of filters that are applied to the input data to extract features. The output of a convolutional layer is a set of feature maps, one for each filter.</p></li>
<li><p><strong>Pooling layers</strong>: these layers are responsible for reducing the dimensionality of the feature maps. A pooling layer is applied to each feature map independently. Average or max pooling are the most common pooling strategies.</p></li>
<li><p><strong>Fully connected layers</strong>: these layers are responsible for rearranging the features extracted by the convolutional layers into a vector of probabilities.</p></li>
</ul>
<p>An example of a convolutional neural network architecture is shown in Figure <a class="reference internal" href="#cnn-architecture"><span class="std std-ref">Simple CNN architecture having all the basic components.
Image source vitalflux.com</span></a>. The network is composed of five convolutional layers, five pooling layers, and tthree fully connected layers. The input of the network is a 224x224 RGB image. The output of the network is a vector of probabilities, one for each class in the dataset (i.e., 1000 in the case of <a class="reference external" href="http://www.image-net.org/">ImageNet</a>).</p>
<figure class="align-default" id="cnn-architecture">
<a class="reference internal image-reference" href="_images/example_cnn.png"><img alt="cnn_architecture" src="_images/example_cnn.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">Simple CNN architecture having all the basic components.
Image source <a class="reference external" href="https://vitalflux.com/different-types-of-cnn-architectures-explained-examples/">vitalflux.com</a></span><a class="headerlink" href="#cnn-architecture" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The convolution operation, together with the pooling operation, is the key component of a convolutional neural network. In the following sections, we will see how these operations work and how they can be used to extract features from images.</p>
<section id="basic-components">
<h2>Basic Components<a class="headerlink" href="#basic-components" title="Link to this heading">#</a></h2>
<p>Convolutional neural networks, also named ConvNets or CNNs, are a class of deep neural networks that have been initially designed to process images. The operations involved in a CNN are inspired by the visual cortex of the human brain. In particular, the convolution operation is inspired by the receptive fields of neurons in the visual cortex.</p>
<section id="convolutional-layers">
<h3>Convolutional Layers<a class="headerlink" href="#convolutional-layers" title="Link to this heading">#</a></h3>
<p>The main component of a CNN is the convolutional layer. In a convolutional layer, a set of filters is applied to the input data to extract <strong>high-level features</strong>.</p>
<p>The convolution operation is defined by a set of parameters:</p>
<ul class="simple">
<li><p>The <strong>filter size</strong> is the size of the filter. The filter size is usually an odd number (e.g., 3x3, 5x5, 7x7).</p></li>
<li><p><strong>Stride</strong> is the number of pixels by which the filter is shifted at each step.</p></li>
<li><p><strong>Padding</strong> is the number of pixels added to each side of the input image. Padding is usually used to preserve the spatial dimension of the input image.</p></li>
</ul>
<p>The convolution operation is applied to each channel of the input image independently. For the moment, let’s consider a single channel image. The convolution operation is defined as follows:</p>
<ol class="arabic simple">
<li><p>The filter is placed on the top-left corner of the input image.</p></li>
<li><p>The element-wise multiplication between the filter and the input image is computed.</p></li>
<li><p>The result of the multiplication is summed up to obtain a single value.</p></li>
<li><p>The filter is shifted by the stride value to the right. If the filter reaches the right side of the image, it is shifted to the left side of the next row.</p></li>
<li><p>Steps 2-4 are repeated until the filter reaches the bottom-right corner of the image.</p></li>
<li><p>The result of the convolution operation is a feature map, a matrix of values that represents the output of the convolution operation.</p></li>
</ol>
<figure class="align-default" id="convolution">
<a class="reference internal image-reference" href="_images/conv_step0.png"><img alt="convolution" src="_images/conv_step0.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">Initial settings of the convolution operation.
On the left, the input image. On the middle, the filter. On the right, the output feature map (empty at the beginning). In the example, stride is 1 and padding is 0 (simpler case).</span><a class="headerlink" href="#convolution" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#convolution"><span class="std std-numref">Fig. 8</span></a> shows the initial settings of the convolution operation. The feature map is empty at the beginning.</p>
<figure class="align-default" id="conv-step-1">
<a class="reference internal image-reference" href="_images/conv_step1.png"><img alt="convolution" src="_images/conv_step1.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">Step 1 of the convolution operation.</span><a class="headerlink" href="#conv-step-1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Step 1</strong>: the filter is placed on the top-left corner of the input image. The element-wise multiplication between the filter and the input image is computed. The result of the multiplication is summed up to obtain a single value. In this example, the result is 6, that is the first value of the feature map as shown in <a class="reference internal" href="#conv-step-1"><span class="std std-numref">Fig. 9</span></a>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\begin{split}
&amp; + 1 \times 1 + 0 \times 2 \\
&amp; + 1 \times 5 + 0 \times 4 \\
&amp; = 6
\end{split}
\end{align}
\end{split}\]</div>
<figure class="align-default" id="conv-step-2">
<a class="reference internal image-reference" href="_images/conv_step2.png"><img alt="convolution" src="_images/conv_step2.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Step 2 of the convolution operation.</span><a class="headerlink" href="#conv-step-2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Step 2</strong>: the filter is shifted by the stride value to the right. If the filter reaches the right side of the image, it is shifted to the left side of the next row. In this example, the filter is shifted by 1 pixel to the right. The result of the convolution operation is 8, that is the second value of the feature map as shown in <a class="reference internal" href="#conv-step-2"><span class="std std-numref">Fig. 10</span></a>.</p>
<figure class="align-default" id="conv-step-n">
<a class="reference internal image-reference" href="_images/conv_stepn.png"><img alt="convolution" src="_images/conv_stepn.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">Last step of the convolution operation.</span><a class="headerlink" href="#conv-step-n" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Step n</strong>: after <span class="math notranslate nohighlight">\(n\)</span> steps, the filter reaches the bottom-right corner of the image. The result of the convolution operation is a feature map, a matrix of values that represents the output of the convolution operation. In this example, the feature map is a 2x2 matrix as shown in <a class="reference internal" href="#conv-step-n"><span class="std std-numref">Fig. 11</span></a>.</p>
<p>The convolution operation is applied to each channel of the input image independently. The result is a set of feature maps, one for each channel of the input image. The number of feature maps is equal to the number of filters in the convolutional layer.</p>
<p><strong>Note</strong>: in the previous example, we considered a <em>fixed</em> filter. In practice, the filters are <em>learned</em> during the training process. The weights of the filters are the parameters of the convolutional layer that the network learns during the training process.</p>
</section>
<section id="pooling-layers">
<h3>Pooling Layers<a class="headerlink" href="#pooling-layers" title="Link to this heading">#</a></h3>
<p>The pooling operation is often used to reduce the dimensionality of the feature maps. The pooling operation is defined by a set of parameters:</p>
<ul class="simple">
<li><p>The <strong>pooling size</strong> is the size of the pooling filter. As for convolutional kernels, the pooling size is usually an odd number (e.g., 3x3, 5x5, 7x7).</p></li>
<li><p><strong>Stride</strong> is the number of pixels by which the pooling filter is shifted at each step.</p></li>
<li><p><strong>Padding</strong> is the number of pixels added to each side of the input image. Padding is usually used to preserve the spatial dimension of the input image.</p></li>
<li><p><strong>Pooling strategy</strong> is the function used to aggregate the values in the pooling filter. The most common pooling strategies are average pooling and max pooling.</p></li>
</ul>
<p>The pooling layer operates in a similar way to the convolutional layer. The pooling filter is placed on the top-left corner of the input image. The pooling operation is applied to each channel of the input image independently. The result of the pooling operation is a feature map, a matrix of values that represents the output of the pooling operation.</p>
<figure class="align-default" id="avg-pooling">
<a class="reference internal image-reference" href="_images/avg_pooling.png"><img alt="avg_pooling" src="_images/avg_pooling.png" style="width: 60%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">Average pooling on a 4x4 matrix with pooling size 2x2 and stride 2. Padding is 0.</span><a class="headerlink" href="#avg-pooling" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#avg-pooling"><span class="std std-numref">Fig. 12</span></a> shows an example of average pooling. The colors help to visualize the pooling operation involving the following steps:</p>
<ul class="simple">
<li><p>The pooling filter is placed on the top-left corner of the input image. The average of the values in the pooling filter is computed. In this example, the average is <span class="math notranslate nohighlight">\((1+2+5+6)/4 = 3.5\)</span>.</p></li>
<li><p>The pooling filter is shifted by the stride value (2) to the right. Again, the average of the values in the pooling filter is computed. In this example, the average is <span class="math notranslate nohighlight">\((3+4+7+8)/4 = 5.5\)</span>.</p></li>
<li><p>Since we reached the right side of the image, the pooling filter is shifted to the left side of the next row. The average of the values in the pooling filter is computed. In this example, the average is <span class="math notranslate nohighlight">\((9+10+13+14)/4 = 11.5\)</span>.</p></li>
<li><p>… the process continues until the pooling filter reaches the bottom-right corner of the image.</p></li>
</ul>
</section>
<section id="stride-and-padding">
<h3>Stride and Padding<a class="headerlink" href="#stride-and-padding" title="Link to this heading">#</a></h3>
<p>To this point, we have seen that the convolution and pooling operations are defined by a set of parameters. In particular, the stride and padding parameters are used to control the spatial dimension of the output feature maps.</p>
<p><strong>Stride</strong> is the number of elements (pixels in an image) by which the filter is shifted at each step. The stride parameter is used to control the spatial dimension of the output feature maps.
Common values for the stride parameter are 1 and 2. A stride of 1 means that the filter is shifted by 1 pixel at each step. A stride of 2 means that the filter is shifted by 2 pixels at each step.</p>
<figure class="align-default" id="stride-step0">
<a class="reference internal image-reference" href="_images/stride_step0.png"><img alt="stride" src="_images/stride_step0.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">Step 1 of the convolution operation with stride 2.</span><a class="headerlink" href="#stride-step0" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="stride-step1">
<a class="reference internal image-reference" href="_images/stride_step1.png"><img alt="stride" src="_images/stride_step1.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">Step 2 of the convolution operation with stride 2.</span><a class="headerlink" href="#stride-step1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="stride-step2">
<a class="reference internal image-reference" href="_images/stride_step2.png"><img alt="stride" src="_images/stride_step2.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Step 3 of the convolution operation with stride 2.</span><a class="headerlink" href="#stride-step2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="stride-step3">
<a class="reference internal image-reference" href="_images/stride_step3.png"><img alt="stride" src="_images/stride_step3.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">Step 4 of the convolution operation with stride 2.</span><a class="headerlink" href="#stride-step3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#stride-step0"><span class="std std-numref">Fig. 13</span></a> shows the initial settings of the convolution operation with stride 2. The filter is placed on the top-left corner of the input image. The result of the convolution operation is 6, that is the first value of the feature map. Similarly the process goes on until the filter reaches the bottom-right corner of the image. <a class="reference internal" href="#stride-step1"><span class="std std-numref">Fig. 14</span></a>, <a class="reference internal" href="#stride-step2"><span class="std std-numref">Fig. 15</span></a>, and <a class="reference internal" href="#stride-step3"><span class="std std-numref">Fig. 16</span></a> show the following steps of the convolution operation.</p>
<p>💡 Notice that the stride is the value by which the filter is shifted at each step <strong>both along the horizontal and vertical dimensions</strong>.</p>
<p><strong>Padding</strong> is the number of elements (pixels in an image) added to each side of the input image. Padding is usually used to preserve the spatial dimension of the input image.
In simple terms, padding is a “border” added to the input image. The value of the padding is usually 0 (zero padding - black border) or 1 (one padding - white border).</p>
<figure class="align-default" id="padding">
<a class="reference internal image-reference" href="_images/padding.png"><img alt="padding" src="_images/padding.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">Padding applied to a 4x4 matrix. Padding size is 1.</span><a class="headerlink" href="#padding" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#padding"><span class="std std-numref">Fig. 17</span></a> shows an example of padding. On the left, the input image. On the right, the padded image where the input matrix is reported in red and the padding is reported in blue. In this example, the padding size is 1.
With an input image of size <span class="math notranslate nohighlight">\(n \times n\)</span>, the output image has size <span class="math notranslate nohighlight">\((n + 2p) \times (n + 2p)\)</span>, where <span class="math notranslate nohighlight">\(p\)</span> is the padding size.
In the example of <a class="reference internal" href="#padding"><span class="std std-numref">Fig. 17</span></a>, the input image is a 4x4 matrix. The padding size is 1. The output image is a 6x6 matrix.</p>
<div class="admonition-computing-the-feature-map-size admonition">
<p class="admonition-title">Computing the feature map size</p>
<p>Once set the parameters of the convolutional layer (filter size, stride, padding), it is possible to compute the size of the output feature map. The size of the output feature map is computed as follows:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\begin{split}
&amp; \text{output size} = \frac{\text{input size} - \text{filter size} + 2 \times \text{padding}}{\text{stride}} + 1
\end{split}
\end{align}
\]</div>
<p>For example, if the input image is a 224x224 RGB image, the filter size is 3x3, the stride is 1, and the padding is 0, the size of the output feature map is:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\begin{split}
&amp; \text{output size} = \frac{224 - 3 + 2 \times 0}{1} + 1 = 222
\end{split}
\end{align}
\]</div>
<p>Intuitively, the output feature map is smaller than the input image because the filter cannot be placed on the edges of the image. The <strong>padding</strong> is used to preserve the spatial dimension of the input image (+ sign in the equation).
The <strong>stride</strong> instead is used to control the reduction of the spatial dimension of the output feature map (i.e., the denominator of the equation).</p>
</div>
<!-- ### Activation Functions

The activation function is a non-linear function that is applied to the output of a layer. The activation function is usually applied after the convolutional and pooling layers. Similarly to fully connected neural networks, the activation function is used to introduce non-linearity in the network. The most common activation functions are:
- **ReLU** (Rectified Linear Unit): $f(x) = max(0, x)$
- **Sigmoid**: $f(x) = \frac{1}{1 + e^{-x}}$
- **Tanh**: $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
- **Softmax**: $f(x) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$

The *softmax* activation function is usually applied to the output of the last layer of the network. The softmax function is used to convert the output of the network into a vector of probabilities. The output of the network is a vector of values between 0 and 1. The sum of the values in the vector is equal to 1. Each value in the vector represents the probability that the input belongs to a specific class. -->
</section>
<section id="receptive-field">
<h3>Receptive Field<a class="headerlink" href="#receptive-field" title="Link to this heading">#</a></h3>
<p>One relevant concept in convolutional neural networks is the <strong>receptive field</strong>. It is defined as the portion of the input image that is visible to a single neuron in the network. The receptive field is usually defined in terms of the number of pixels in the input image. For example, a receptive field of 3x3 means that the neuron can “see” a 3x3 portion of the input image.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/receptive_field.png"><img alt="receptive_field" src="_images/receptive_field.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">Example of receptive field in a CNN.</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#id2"><span class="std std-numref">Fig. 18</span></a> shows an example of receptive field in a CNN. The receptive field of the neuron in the third layer is 3x3 when considering the second layer. If we consider the input image (e.g., layer 1), the receptive field of the same neuron would be 9x9 (the entire image is 5x5 so the neuron can “see” the whole image).</p>
<p>Intuitively, the receptive field defines the region of the input image that has contributed to the activation of a neuron in the network.</p>
</section>
</section>
<section id="common-cnn-architectures">
<h2>Common CNN Architectures<a class="headerlink" href="#common-cnn-architectures" title="Link to this heading">#</a></h2>
<p>Since the introduction of the first CNN architecture in the 1980s, many different architectures have been proposed. They differ in terms of the number of layers, the number of filters, the pooling strategy, etc., but also in terms of architectural choices that have been made to improve both the performance and the training process of the network.</p>
<section id="lenet-5">
<h3>LeNet-5<a class="headerlink" href="#lenet-5" title="Link to this heading">#</a></h3>
<figure class="align-default" id="lenet5">
<a class="reference internal image-reference" href="_images/lenet5.jpeg"><img alt="lenet5" src="_images/lenet5.jpeg" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">LeNet-5 architecture.</span><a class="headerlink" href="#lenet5" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>LeNet-5 <span id="id3">[<a class="reference internal" href="references.html#id5" title="Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.">LBD+89</a>]</span> is the first CNN architecture proposed by Yann LeCun and colleagues. The network was designed to recognize handwritten digits. The network is composed of 7 layers: 3 convolutional layers, 2 pooling layers, and 2 fully connected layers. The input of the network is a 32x32 grayscale image. The output of the network is a vector of probabilities, one for each class in the dataset (i.e., 10 in the case of MNIST).</p>
<p>We can see that since the first convolutional layer, the number of filters (number of channels in the feature maps) increases from 1 (greyscale image) to 6. On the other hand, the spatial dimension of the feature maps decreases from 32x32 to 28x28. This is one of the main characteristics shared by many CNN architectures: going deeper in the network, the number of filters increases while the spatial dimension of the feature maps decreases.</p>
</section>
<section id="alexnet">
<h3>AlexNet<a class="headerlink" href="#alexnet" title="Link to this heading">#</a></h3>
<figure class="align-default" id="alex-net">
<a class="reference internal image-reference" href="_images/alex_net.png"><img alt="alex_net" src="_images/alex_net.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">AlexNet architecture.</span><a class="headerlink" href="#alex-net" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>AlexNet <span id="id4">[<a class="reference internal" href="references.html#id6" title="Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 2012.">KSH12</a>]</span> is a CNN architecture proposed by Alex Krizhevsky and colleagues. The network was designed to classify images in the ImageNet dataset. The network is composed of 8 layers: 5 convolutional layers, 3 fully connected layers. The input of the network is a 224x224 RGB image. The output of the network is a vector of probabilities, one for each class in the dataset (i.e., 1000 in the case of ImageNet).</p>
<p>We can see that the network is composed of two groups of layers. The first group is composed of 5 convolutional layers and 3 pooling layers. The second group is composed of 3 fully connected layers. The first group is responsible for extracting features from the input image. The second group is responsible for classifying the input image.</p>
<p>It is worth mentioning that, at the time of its introduction, AlexNet was the first CNN architecture to use ReLU as activation function and dropout as regularization technique. AlexNet is also the winning entry of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012, where it reduced the top-5 error by a large margin compared to the previous state-of-the-art.</p>
</section>
<section id="resnet">
<h3>ResNet<a class="headerlink" href="#resnet" title="Link to this heading">#</a></h3>
<figure class="align-default" id="res-block">
<a class="reference internal image-reference" href="_images/res_block.png"><img alt="res_block" src="_images/res_block.png" style="width: 30%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">ResBlock in ResNet architecture.</span><a class="headerlink" href="#res-block" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>ResNet <span id="id5">[<a class="reference internal" href="references.html#id7" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770–778. 2016.">HZRS16a</a>]</span> is a CNN architecture proposed by Kaiming He and colleagues. Similarly to AlexNet, the network was designed to classify images in the ImageNet dataset. This network is the first to introduce the concept of <strong>residual learning</strong> that allows training <strong>very deep</strong> networks. There are different versions of ResNet, with 50, 101, 152, and 200 layers. The network is composed of a concatenation of <strong>residual blocks</strong>. Each residual block is composed of two convolutional layers, batch normalization, ReLU activation function, and a shortcut connection (i.e., the residual connection).</p>
<p><a class="reference internal" href="#res-block"><span class="std std-numref">Fig. 21</span></a> shows an example of residual block. It is composed of two convolutional layers and a shortcut connection. The shortcut connection is used to add the input feature map to the output of the second convolutional layer. The output of the residual block is the sum of the input feature map and the output of the second convolutional layer. Batch normalization is applied after each convolutional layer and ReLU is used as activation function both after the first batch normalization and after the shortcut connection.</p>
<div class="note dropdown admonition">
<p class="admonition-title">Residual Learning</p>
<p>Residual learning is a technique in training <strong>exceptionally deep</strong> neural networks. The fundamental concept involves incorporating a shortcut connection between the input and the output of a layer. Specifically, the output of the layer is formed by summing the input with the output of the layer.</p>
<p>The rationale behind residual learning lies in the belief that as layers are stacked in a network, the model can learn abstract features that are more useful for the downstream task than the shallower features acquired by less complex networks. However, this stacking of layers can bring to the <strong>vanishing gradient problem</strong>, a common obstacle in deep neural networks. The vanishing gradient problem manifests when the gradient of the loss function diminishes significantly with an increase in the number of layers. This slowdown in the gradient severely affect the training process.</p>
<p>Residual learning provides an elegant solution to the vanishing gradient problem by introducing a <strong>shortcut connection</strong> that directly links the input to the output of a layer. This shortcut connection facilitates an <strong>uninterrupted flow</strong> of the gradient from the output back to the input of the layer, effectively mitigating the challenges posed by the vanishing gradient problem. As a result, residual learning empowers the training of extremely deep networks, enabling them to capture intricate patterns and representations essential for complex tasks.</p>
<figure class="align-default" id="residual-learning">
<a class="reference internal image-reference" href="_images/residual_learning.gif"><img alt="residual_learning" src="_images/residual_learning.gif" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">Residual learning.</span><a class="headerlink" href="#residual-learning" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
<p>Many other CNN architectures have been proposed in the last years. Here are some references if the reader is interested in learning more about CNN architectures:</p>
<ul class="simple">
<li><p>VGG <span id="id6">[<a class="reference internal" href="references.html#id8" title="Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.">SZ14</a>]</span>, introduced the concept of using small convolutional filters (3x3) with stride 1 and padding 1.</p></li>
<li><p>DenseNet <span id="id7">[<a class="reference internal" href="references.html#id9" title="Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 4700–4708. 2017.">HLVDMW17</a>]</span>, introduced the concept of dense blocks, where each layer is connected to all the previous layers.</p></li>
<li><p>Inception <span id="id8">[<a class="reference internal" href="references.html#id10" title="Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, 1–9. 2015.">SLJ+15</a>]</span>, introduced the concept of inception modules, where the input is processed by different convolutional filters and the output is concatenated.</p></li>
<li><p>MobileNet <span id="id9">[<a class="reference internal" href="references.html#id11" title="Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.">HZC+17</a>]</span>, introduced the concept of depthwise separable convolution, where the convolution operation is split into two separate operations: depthwise convolution and pointwise convolution.</p></li>
<li><p>EfficientNet <span id="id10">[<a class="reference internal" href="references.html#id12" title="Mingxing Tan and Quoc Le. Efficientnet: rethinking model scaling for convolutional neural networks. In International conference on machine learning, 6105–6114. PMLR, 2019.">TL19</a>]</span>, introduced the concept of compound scaling, where the depth, width, and resolution of the network are scaled together.</p></li>
</ul>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="convnets-for-audio-data">
<h1>ConvNets for Audio Data<a class="headerlink" href="#convnets-for-audio-data" title="Link to this heading">#</a></h1>
<p>The considerations made for image data are also valid for audio data. Audio data are characterized by high dimensionality and spatial correlation. For example, a 1-second audio clip sampled at 44.1 kHz is represented as a 44100-dimensional vector. Similarly to images, the use of standard fully connected neural networks is not feasible for audio data.</p>
<div class="admonition-exercise-fc-for-audio-data admonition">
<p class="admonition-title">Exercise: FC for Audio Data</p>
<p>Consider a 1-second audio clip sampled at 44.1 kHz. The audio clip is represented as a 44100-dimensional vector. Suppose we want to use a fully connected neural network to classify the audio clip. The network is composed of 3 fully connected layers with 1000 units each. How many parameters does the network have?</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\begin{split}
&amp; \text{input size} = 44100 \\
&amp; \text{output size} = 1000 \\
&amp; \text{layers} = 3 \times 1000 \\
&amp; \text{parameters} = 44100 \times 1000 + 1000 \times 1000 + 1000 \times 1000 = 45100000 = 45.1 \text{M}
\end{split}
\end{align}
\end{split}\]</div>
<p>The network has 45.1 million parameters. This is a huge number of parameters. Training such a network would require a lot of data and a lot of computational resources.
Also, we are just considering a 1-second audio clip. In practice, audio clips used for classification tasks are usually longer than 1 second. For example, the audio clips in the <a class="reference external" href="https://research.google.com/audioset/">AudioSet</a> dataset are 10 seconds long, sampled at 16 kHz. This means that each audio clip is represented as a 160000-dimensional vector. The number of parameters of the network would increase exponentially.</p>
</div>
<section id="d-convolutional-layers">
<h2>1-D Convolutional Layers<a class="headerlink" href="#d-convolutional-layers" title="Link to this heading">#</a></h2>
<p>The convolution operation seen in the previous section can be applied to 1-dimensional data. In this case, the kernel is a 1-dimensional vector that slides along the input data. The output of the convolution operation is a 1-dimensional vector, the feature map.</p>
<p>The same concepts seen for 2-dimensional data are valid for 1-dimensional data. The convolution operation is defined by the same set of parameters: <strong>filter size</strong>, <strong>stride</strong>, and <strong>padding</strong>. The result is a set of feature maps, one for each channel of the input data.</p>
<figure class="align-default" id="conv-1d">
<a class="reference internal image-reference" href="_images/conv_1d.png"><img alt="conv_1d" src="_images/conv_1d.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 23 </span><span class="caption-text">1-D convolution.</span><a class="headerlink" href="#conv-1d" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The convolution operation is applied to each channel of the input data independently. Similarly to the 2D-case, Convolutional Neural Networks are composed of a set of convolutional layers, pooling layers, and fully connected layers.</p>
<p>The main difference between 1D and 2D convolutional layers is that 1D convolutional layers are used to process 1-dimensional data (e.g., audio data) while 2D convolutional layers are used to process 2-dimensional data (e.g., image data).</p>
<p>💡 Note that, the number of channels and the number of dimensions are two different concepts. <strong>Dimensionality</strong> refers to the number of dimensions of the input data. <strong>Channels</strong> refers to the number of channels of the input data. For example, an audio clip recorded in stereo conditions (e.g., with two microphones) is a 1-dimensional signal with 2 channels. An audio clip recorded in mono conditions (e.g., with one microphone) is a 1-dimensional signal with 1 channel.
Visual data usually have 3 channels (RGB images) and 2 dimensions (width and height).</p>
<section id="convolutional-layers-for-temporal-patterns">
<h3>Convolutional Layers for Temporal Patterns<a class="headerlink" href="#convolutional-layers-for-temporal-patterns" title="Link to this heading">#</a></h3>
<p>1D convolutional layers are usually used to extract <strong>temporal patterns</strong> from audio data. For example, a 1D convolutional layer can be used to extract the temporal patterns of a specific instrument in a music track.</p>
<figure class="align-default" id="d-conv-stride1">
<a class="reference internal image-reference" href="_images/1d_conv_stride1.gif"><img alt="1d_conv_stride1" src="_images/1d_conv_stride1.gif" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 24 </span><span class="caption-text">Example of 1D convolution with stride 1. Source: <a class="reference external" href="https://e2eml.school/convolution_one_d.html">e2eml.school</a></span><a class="headerlink" href="#d-conv-stride1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="d-conv-stride2">
<a class="reference internal image-reference" href="_images/1d_conv_stride2.gif"><img alt="1d_conv_stride2" src="_images/1d_conv_stride2.gif" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 25 </span><span class="caption-text">Example of 1D convolution with stride 2. Source: <a class="reference external" href="https://e2eml.school/convolution_one_d.html">e2eml.school</a></span><a class="headerlink" href="#d-conv-stride2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="d-conv-stride4">
<a class="reference internal image-reference" href="_images/1d_conv_stride4.gif"><img alt="1d_conv_stride4" src="_images/1d_conv_stride4.gif" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 26 </span><span class="caption-text">Example of 1D convolution with stride 4. Source: <a class="reference external" href="https://e2eml.school/convolution_one_d.html">e2eml.school</a></span><a class="headerlink" href="#d-conv-stride4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#d-conv-stride1"><span class="std std-numref">Fig. 24</span></a>, <a class="reference internal" href="#d-conv-stride2"><span class="std std-numref">Fig. 25</span></a>, and <a class="reference internal" href="#d-conv-stride4"><span class="std std-numref">Fig. 26</span></a> show an example of 1D convolution with stride 1, 2, and 4 respectively. The input data is a 1-dimensional vector of 25 elements. The filter is a 1-dimensional vector of 9 elements. The output of the convolution operation is a 1-dimensional vector of 16 elements. The number of elements in the output vector is computed as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\begin{split}
&amp; \text{o} = \frac{\text{input size} - \text{filter size} + 2 \times \text{padding}}{\text{stride}} + 1 \\
&amp; \text{o (s=1)} = \frac{25 - 9 + 2 \times 0}{1} + 1 = 17 \\
&amp; \text{o (s=2)} = \frac{25 - 9 + 2 \times 0}{2} + 1 = 9 \\
&amp; \text{o (s=4)} = \frac{25 - 9 + 2 \times 0}{4} + 1 = 5 \\
\end{split}
\end{align}
\end{split}\]</div>
<div class="note dropdown admonition">
<p class="admonition-title">Multichannel 1D Convolution</p>
<p>One dimensional data may have multiple channels (e.g., stereo audio data). Similarly, an electroencephalograpy signal may come with 128 channels.  A multichannel signal share a common dimension (e.g., the time axis) In this case each channel is processed independently by a 1D convolutional layer.</p>
<p>We should note that, we cannot expect that all data channels share the same temporal patterns. For example, in a stereo audio signal, the left and right channels may contain different instruments. For this reason, it is common to use a different set of filters for each channel.</p>
<figure class="align-default" id="d-multichannel">
<a class="reference internal image-reference" href="_images/1d_multichannel.gif"><img alt="1d_multichannel" src="_images/1d_multichannel.gif" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 27 </span><span class="caption-text">Example of multichannel 1D convolution. Source: <a class="reference external" href="https://e2eml.school/convolution_one_d.html">e2eml.school</a></span><a class="headerlink" href="#d-multichannel" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#d-multichannel"><span class="std std-numref">Fig. 27</span></a> shows an example of multichannel 1D convolution. The input data is a 1-dimensional vector with 3 channels, we use 3 different filters to process each channel independently. The result of a multichannel convolution is a single channel output. Because the sliding dot product covers all the channels at once, they all get summed together and reduced down to a single channel. <a class="reference external" href="https://e2eml.school/convolution_one_d.html">in-depth explanation</a></p>
</div>
</section>
</section>
<section id="spectrogram-representation">
<h2>Spectrogram Representation<a class="headerlink" href="#spectrogram-representation" title="Link to this heading">#</a></h2>
<p>Audio data are usually represented as a 1-dimensional vector that contains the amplitude of the signal at each time step.</p>
<figure class="align-default" id="waveform">
<a class="reference internal image-reference" href="_images/waveform.png"><img alt="waveform" src="_images/waveform.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 28 </span><span class="caption-text">Example of audio waveform for a 30-second audio clip.</span><a class="headerlink" href="#waveform" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#waveform"><span class="std std-numref">Fig. 28</span></a> shows an example of audio waveform for a 30-second audio clip. The x-axis represents the time in seconds. The y-axis represents the amplitude of the signal. The audio clip is sampled at 16 kHz.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>How many samples are there in a 30-second audio clip sampled at 16 kHz?</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
&amp; \text{samples} = 30 \times 16000 = 480000
\end{align}
\]</div>
</div>
<p>However, the amplitude of the signal is not the only information that we can extract from an audio clip. For example, we can extract the <strong>frequency</strong> of the signal. The frequency of a signal is the number of cycles of a wave that occur in a second. The frequency of a signal is measured in Hertz (Hz). The frequency of a signal is related to the pitch of the sound. For example, a low-pitched sound has a low frequency while a high-pitched sound has a high frequency.</p>
<p>We can represent the frequency of a signal using a <strong>spectrogram</strong>. A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. A spectrogram is usually represented as a 2-dimensional matrix where the x-axis represents the time in seconds and the y-axis represents the frequency in Hz. The color of each pixel represents the amplitude of the signal at a specific time and frequency bin.</p>
<figure class="align-default" id="spectrogram">
<a class="reference internal image-reference" href="_images/spectrogram.png"><img alt="spectrogram" src="_images/spectrogram.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 29 </span><span class="caption-text">Example of spectrogram for a 30-second audio clip.</span><a class="headerlink" href="#spectrogram" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#spectrogram"><span class="std std-numref">Fig. 29</span></a> shows an example of spectrogram for a 30-second audio clip. The x-axis represents the time in seconds. The y-axis represents the frequency in Hz. The color of each pixel represents the amplitude of the signal at a specific time and frequency bin.</p>
<section id="short-time-fourier-transform">
<h3>Short-Time Fourier Transform<a class="headerlink" href="#short-time-fourier-transform" title="Link to this heading">#</a></h3>
<p>The spectrogram is computed using the <strong>Short-Time Fourier Transform</strong> (STFT). The STFT is a technique to compute the Fourier transform of a signal over a short window of time. The Fourier transform is a mathematical operation that decomposes a signal into its constituent frequencies. The Fourier transform is defined as follows:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
&amp; X(f) = \int_{-\infty}^{\infty} x(t) e^{-i 2 \pi f t} dt
\end{align}
\]</div>
<p>where <span class="math notranslate nohighlight">\(x(t)\)</span> is the signal, <span class="math notranslate nohighlight">\(X(f)\)</span> is the Fourier transform of the signal, and <span class="math notranslate nohighlight">\(f\)</span> is the frequency. However, the Fourier transform is defined for continuous signals. In practice, we usually deal with discrete signals. For this reason, we use the <strong>Discrete Fourier Transform</strong> (DFT) instead of the Fourier transform. The DFT is defined as follows:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
&amp; X(k) = \sum_{n=0}^{N-1} x(n) e^{-i 2 \pi k n / N}
\end{align}
\]</div>
<p>where <span class="math notranslate nohighlight">\(x(n)\)</span> is the signal, <span class="math notranslate nohighlight">\(X(k)\)</span> is the DFT of the signal, <span class="math notranslate nohighlight">\(k\)</span> is the frequency, and <span class="math notranslate nohighlight">\(N\)</span> is the number of samples in the signal. The DFT is a discrete version of the Fourier transform.</p>
<figure class="align-default" id="stft">
<a class="reference internal image-reference" href="_images/iscola_stft.png"><img alt="stft" src="_images/iscola_stft.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 30 </span><span class="caption-text">Example of STFT process.</span><a class="headerlink" href="#stft" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#stft"><span class="std std-numref">Fig. 30</span></a> shows an example of STFT process. The input signal is a 1-dimensional vector (e.g., the audio file) represented as <span class="math notranslate nohighlight">\(x(n)\)</span>.
The STFT is computed by applying the DFT to a windowed portion of the signal. The window length in the image is <span class="math notranslate nohighlight">\(M\)</span> and the window shift is <span class="math notranslate nohighlight">\(R\)</span>. <span class="math notranslate nohighlight">\(L\)</span> is the overlap between two consecutive windows. The output of the STFT is a 2-dimensional matrix (e.g., the spectrogram) represented as <span class="math notranslate nohighlight">\(X(m, k)\)</span>.</p>
<figure class="align-default" id="stft-animation">
<a class="reference internal image-reference" href="_images/stft_animation.gif"><img alt="stft_animation" src="_images/stft_animation.gif" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 31 </span><span class="caption-text">Example of STFT process in action.</span><a class="headerlink" href="#stft-animation" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#stft-animation"><span class="std std-numref">Fig. 31</span></a> shows an example of STFT process in action. The input signal is processed using a given window length and window shift. The output of the STFT is a 2-dimensional matrix (e.g., the spectrogram).</p>
<p>This section is not intended to be a comprehensive introduction to the STFT. For a more in-depth explanation, we refer the reader to specific <a class="reference external" href="https://www.youtube.com/playlist?list=PLn0OLiymPak2jxGCbWrcgmXUtt9Lbjj_A">online resources</a>.</p>
</section>
<section id="mel-spectrogram">
<h3>Mel-Spectrogram<a class="headerlink" href="#mel-spectrogram" title="Link to this heading">#</a></h3>
<p>The spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. However, the human ear does not perceive all frequencies equally. For example, we are more sensitive to frequencies between 2 kHz and 5 kHz than to frequencies between 0 Hz and 1 kHz. For this reason, the spectrogram is usually converted into a <strong>Mel-spectrogram</strong>. A Mel-spectrogram is a spectrogram where the frequencies are converted into the Mel scale. The Mel scale is a perceptual scale of pitches judged by listeners to be equal in distance from one another. The Mel scale is defined as follows:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
&amp; m = 2595 \log_{10} \left( 1 + \frac{f}{700} \right)
\end{align}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is the frequency in Hz and <span class="math notranslate nohighlight">\(m\)</span> is the frequency in Mel. The Mel scale is a non-linear transformation of the frequency scale. The Mel scale is used to convert the spectrogram into a Mel-spectrogram. The Mel-spectrogram is usually represented as a 2-dimensional matrix where the x-axis represents the time in seconds and the y-axis represents the frequency in Mel. The color of each pixel represents the amplitude of the signal at a specific time and frequency bin.</p>
<figure class="align-default" id="mel">
<a class="reference internal image-reference" href="_images/mel.png"><img alt="mel" src="_images/mel.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 32 </span><span class="caption-text">Example of Mel-spectrogram for a 30-second audio clip.</span><a class="headerlink" href="#mel" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#mel"><span class="std std-numref">Fig. 32</span></a> shows an example of Mel-spectrogram for a 30-second audio clip. <em>Mel-spectrogram</em> is usually used as input to Convolutional Neural Networks for audio data.
It is processed as a 2-dimensional image and similar architectures to those used for image data can be used.</p>
</section>
</section>
<section id="audio-cnn-architectures">
<h2>Audio CNN Architectures<a class="headerlink" href="#audio-cnn-architectures" title="Link to this heading">#</a></h2>
<p>Even if the concepts seen for image data are also valid for audio data, there are some differences between the two domains. For example, the receptive field of a neuron in a CNN for audio data may be much larger than the receptive field of a neuron in a CNN for image data. This is due to the fact that the temporal and spatial dimensions follow different rules and thus require different considerations.</p>
<!-- ### VGGish

### WaveNet [link](https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/)

### YAMNet [link](https://blog.tensorflow.org/2021/03/transfer-learning-for-audio-data-with-yamnet.html) -->
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="implementing-a-cnn-with-pytorch">
<h1>Implementing a CNN with PyTorch<a class="headerlink" href="#implementing-a-cnn-with-pytorch" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://pytorch.org/">PyTorch</a> is an open source machine learning framework that accelerates the path from research prototyping to production deployment.
PyTorch is developed by Facebook’s AI Research lab (FAIR) and is used in many research projects.</p>
<p>PyTorch is a Python package that allows the design of neural networks at both high and low levels. It offers different layers, optimizers, loss functions, etc.
PyTorch also offers a set of tools to facilitate the training process of neural networks (e.g., data loaders, callbacks, etc.).</p>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class is an abstract class representing a dataset. The <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class is used to represent a collection of data samples. For example in <strong>supervised learning</strong> each sample is a pair <code class="docutils literal notranslate"><span class="pre">(input,</span> <span class="pre">target)</span></code>. The <code class="docutils literal notranslate"><span class="pre">input</span></code> is the input data of the sample (e.g., an image, an audio clip, etc.). The <code class="docutils literal notranslate"><span class="pre">target</span></code> is the target data of the sample (e.g., a label, a class, etc.).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class is an abstract class. This means that it cannot be instantiated directly. Instead, we need to create a subclass that inherits from the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class. The subclass must implement two methods: <code class="docutils literal notranslate"><span class="pre">__len__</span></code> and <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>. The <code class="docutils literal notranslate"><span class="pre">__len__</span></code> method returns the size of the dataset. The <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> method returns the sample at the given index.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="c1"># Initialize the dataset</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Return the size of the dataset</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="c1"># Return the sample at the given index</span>
        <span class="k">pass</span>
</pre></div>
</div>
<p>Specialized libraries exists in both Computer Vision and Audio Processing domains, <a class="reference external" href="https://pytorch.org/vision/stable/index.html">torchvision</a> and <a class="reference external" href="https://pytorch.org/audio/stable/index.html">torchaudio</a>. These libraries provide a set of datasets and utilities to facilitate the design of neural networks for Computer Vision and Audio Processing tasks.</p>
<section id="id11">
<h3>MNIST<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p>The <a class="reference external" href="https://pytorch.org/vision/stable/datasets.html#mnist">MNIST</a> dataset is a dataset of handwritten digits. The dataset is composed of 60000 training samples and 10000 test samples. Each sample is a 28x28 grayscale image and the target is a digit between 0 and 9.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>

<span class="c1"># Download the dataset</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;data/MNIST/&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 

<span class="c1"># ! train_dataset is a Dataset object</span>
<span class="c1"># ! we do not need to implement the subclass</span>

<span class="c1"># Get the size of the dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))</span> <span class="c1"># 60000</span>

<span class="c1"># Get the sample at index 0</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> 
<span class="c1"># (&lt;PIL.Image.Image image mode=L size=28x28 at 0x7F9C70B726E0&gt;, 5)</span>
</pre></div>
</div>
</section>
<section id="italian-intent-classification-italic">
<h3>ITALian Intent Classification (ITALIC)<a class="headerlink" href="#italian-intent-classification-italic" title="Link to this heading">#</a></h3>
<p>The ITALIC dataset is a collection of around 16,000 audio recordings of Italian sentences. Each sentence has an associated intent, which is a label that describes the purpose of the sentence. The dataset is available on <a class="reference external" href="https://zenodo.org/records/8040649">Zenodo</a>. The dataset is also available through the <a class="reference external" href="https://huggingface.co/docs/datasets/index">huggingface datasets</a> library.</p>
<p>This library provides a set of datasets that can be used for different tasks (e.g., image or audio classification).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># Please be sure to use use_auth_token=True and to set the access token</span>
<span class="c1"># using huggingface-cli login</span>
<span class="c1"># or follow https://huggingface.co/docs/hub/security-tokens </span>

<span class="c1"># configs &quot;hard_speaker&quot; and &quot;hard_noisy&quot; are also available (to substitute &quot;massive&quot;)</span>
<span class="n">italic</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;RiTA-nlp/ITALIC&quot;</span><span class="p">,</span> <span class="s2">&quot;massive&quot;</span><span class="p">,</span> <span class="n">use_auth_token</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
<span class="n">italic_train</span> <span class="o">=</span> <span class="n">italic</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
<span class="n">italic_valid</span> <span class="o">=</span> <span class="n">italic</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">]</span>
<span class="n">italic_test</span>  <span class="o">=</span> <span class="n">italic</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span>

<span class="c1"># Get the size of the dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">italic_train</span><span class="p">))</span> <span class="c1"># 11514</span>

<span class="c1"># Get the sample at index 0</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">italic_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="c1"># {&#39;id&#39;: 1, &#39;age&#39;: 27, &#39;gender&#39;: &#39;male&#39;, &#39;region&#39;: &#39;abruzzo&#39;, &#39;nationality&#39;: &#39;italiana&#39;, &#39;lisp&#39;: &#39;nessuno&#39;, &#39;education&#39;: &#39;master&#39;, &#39;speaker_id&#39;: 72, &#39;environment&#39;: &#39;silent&#39;, &#39;device&#39;: &#39;phone&#39;, &#39;scenario&#39;: &#39;alarm&#39;, &#39;field&#39;: &#39;close&#39;, &#39;intent&#39;: &#39;alarm_set&#39;, &#39;utt&#39;: &#39;svegliami alle nove di mattina venerdì&#39;, &#39;audio&#39;: {&#39;path&#39;: &#39;/home/mlaquatra/.cache/huggingface/datasets/downloads/extracted/7f25fd6b6a74a983b3f0c3ea3ec3768f916c1fd6a84cd344bc1cedbd9249e698/zenodo_dataset/recordings/1.wav&#39;, &#39;array&#39;: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), &#39;sampling_rate&#39;: 16000}}</span>
</pre></div>
</div>
<p>Dataset objects contain a convenient way to access the data. Also a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class has the flexibility to provide different views of the data. If the model we are implementing requires a specific format for the data, we can implement a <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> method that returns the data in the required format.</p>
<p>For example, we can implement a PyTorch <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class for the ITALIC dataset that returns a tuple or a dictionary containing the audio data and the target data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="k">class</span> <span class="nc">ITALICDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;tuple&quot;</span><span class="p">):</span>
        <span class="c1"># Load the dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;RiTA-nlp/ITALIC&quot;</span><span class="p">,</span> <span class="n">split</span><span class="p">,</span> <span class="n">use_auth_token</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="nb">format</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Return the size of the dataset</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="c1"># Return the sample at the given index</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="c1"># Get the audio data</span>
        <span class="n">audio</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;array&quot;</span><span class="p">]</span>
        <span class="c1"># Get the target data</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;intent&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;tuple&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">audio</span><span class="p">,</span> <span class="n">target</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;dict&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;audio&quot;</span><span class="p">:</span> <span class="n">audio</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="n">target</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Format not supported&quot;</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ITALICDataset</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="dataloader">
<h2>DataLoader<a class="headerlink" href="#dataloader" title="Link to this heading">#</a></h2>
<p>Once we have defined a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class, we can use it to create a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> object. As we have seen in previous chapters, the training of a complex neural network requires the generation of batches of data. The <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> class is used to generate batches of data from a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># Create a DataLoader object</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Iterate over the batches</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
    <span class="c1"># Get the input data</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Get the target data</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># Do something with the data</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>Depending on the split of data (e.g., train, validation, test), we can create different <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> objects. This is useful because, for training, we usually want to shuffle the data while for validation and test we do not want to shuffle the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># Create a DataLoader object for training</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Create a DataLoader object for validation</span>
<span class="n">valid_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="o">...</span>
</pre></div>
</div>
<p>The mix of <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> classes is a powerful tool to manage the data in a machine learning project. The <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class is used to represent a collection of data samples. The <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> class is used to generate batches of data from a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> object.</p>
</section>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">nn</span></code> module <a class="reference external" href="https://pytorch.org/docs/stable/nn.html">[ref]</a> provides a set of tools to design neural networks. The <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class is an abstract class representing a neural network. It is used to represent a neural network. The <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class is an abstract class. This means that it cannot be instantiated directly. Instead, we need to create a subclass that inherits from the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class.</p>
<p>Similar to the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class, the subclass must implement two methods: <code class="docutils literal notranslate"><span class="pre">__init__</span></code> and <code class="docutils literal notranslate"><span class="pre">forward</span></code>. The <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method is used to initialize the layers of the network. The <code class="docutils literal notranslate"><span class="pre">forward</span></code> method is used to define the forward pass of the network.</p>
<p>💡 Note that, the forward pass of the network is defined by the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method. The backward pass is automatically computed by PyTorch using the <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html">autograd</a> module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="c1"># Initialize the layers of the network</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Define the forward pass of the network</span>
        <span class="k">pass</span>
</pre></div>
</div>
<section id="id12">
<h3>Convolutional Layers<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">nn</span></code> module provides a set of layers that can be used to design a neural network. For example, the <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> layer is used to implement a 2-dimensional convolutional layer. The <code class="docutils literal notranslate"><span class="pre">nn.Conv1d</span></code> layer is used to implement a 1-dimensional convolutional layer. The <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> layer is used to implement a fully connected layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># 2D convolutional layer</span>
<span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 1D convolutional layer</span>
<span class="n">conv1d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Fully connected layer</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="activation-functions">
<h3>Activation Functions<a class="headerlink" href="#activation-functions" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">nn</span></code> module also provides a set of activation functions that can be used to design a neural network. For example, the <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> layer is used to implement the ReLU activation function. The <code class="docutils literal notranslate"><span class="pre">nn.Sigmoid</span></code> layer is used to implement the Sigmoid activation function. The <code class="docutils literal notranslate"><span class="pre">nn.Tanh</span></code> layer is used to implement the Tanh activation function. The <code class="docutils literal notranslate"><span class="pre">nn.Softmax</span></code> layer is used to implement the Softmax activation function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># ReLU activation function</span>
<span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

<span class="c1"># Sigmoid activation function</span>
<span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>

<span class="c1"># Tanh activation function</span>
<span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>

<span class="o">...</span>
</pre></div>
</div>
<p>Similarly, there exist other layers already implemented in PyTorch.</p>
<ul class="simple">
<li><p><strong>Pooling layers</strong>: <code class="docutils literal notranslate"><span class="pre">nn.MaxPool2d</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.MaxPool1d</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.AvgPool2d</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.AvgPool1d</span></code></p></li>
<li><p><strong>Normalization layers</strong>: <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm2d</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm1d</span></code></p></li>
<li><p><strong>Dropout layers</strong>: <code class="docutils literal notranslate"><span class="pre">nn.Dropout2d</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.Dropout1d</span></code></p></li>
<li><p><strong>Embedding layers</strong>: <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code></p></li>
<li><p>…</p></li>
</ul>
</section>
<section id="loss-functions">
<h3>Loss Functions<a class="headerlink" href="#loss-functions" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">nn</span></code> module also provides a set of loss functions that can be used to design a neural network. For example, the <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code> layer is used to implement the Cross Entropy loss function that is the standard loss function for classification tasks. The <code class="docutils literal notranslate"><span class="pre">nn.MSELoss</span></code> layer is used to implement the Mean Squared Error loss function that is the standard loss function for regression tasks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">predicitons</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># 10 samples, 5 classes</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># 10 samples, 5 classes</span>

<span class="c1"># Cross Entropy loss function</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">predicitons</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="optimizers-and-schedulers">
<h3>Optimizers and Schedulers<a class="headerlink" href="#optimizers-and-schedulers" title="Link to this heading">#</a></h3>
<p>When training our model, we need to define an <strong>optimizer</strong> and a <strong>scheduler</strong>.</p>
<p>The <strong>optimizer</strong> is used to update the parameters of the network. The optimizer is defined by the <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> module <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">[ref]</a>. The <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> module provides a set of optimizers that can be used to train a neural network. For example, the <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD</span></code> optimizer is used to implement the Stochastic Gradient Descent optimizer. The <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam</span></code> optimizer is used to implement the Adam optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="c1"># Stochastic Gradient Descent optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>The <strong>scheduler</strong> is used to update the learning rate of the optimizer. Usually, we don’t want to use a fixed learning rate. Instead, we want to decrease the learning rate during the training process. The scheduler is defined by the <code class="docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler</span></code> module <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">[ref]</a>. The <code class="docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler</span></code> module provides a set of schedulers that can be used to train a neural network. For example, the <code class="docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.StepLR</span></code> scheduler is used to implement the StepLR scheduler. The <code class="docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.ReduceLROnPlateau</span></code> scheduler is used to implement the ReduceLROnPlateau scheduler.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="c1"># StepLR scheduler</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Link to this heading">#</a></h2>
<p>The training process of a neural network is usually composed of the following steps:</p>
<ol class="arabic simple">
<li><p><strong>Forward pass</strong>: the input data is processed by the network to obtain the output data.</p></li>
<li><p><strong>Loss computation</strong>: the output data is compared with the target data to compute the loss.</p></li>
<li><p><strong>Backward pass</strong>: the loss is used to compute the gradients of the network parameters.</p></li>
<li><p><strong>Parameters update</strong>: the gradients are used to update the parameters of the network.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Iterate over the batches</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># Set the model in training mode</span>
<span class="c1"># Why?</span>

<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
    <span class="c1"># Get the input data</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Get the target data</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># Forward pass</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
    <span class="c1"># Loss computation</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="c1"># Backward pass</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Parameters update</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># Reset the gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<p>💡 The <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code> method is used to reset the gradients of the network parameters. This is necessary because PyTorch accumulates the gradients on subsequent backward passes. This means that, if we do not reset the gradients, the gradients will be accumulated on subsequent backward passes.</p>
<p>💡 The <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> method is used to update the parameters of the network. This is necessary because PyTorch does not update the parameters automatically. This means that, if we do not update the parameters, the parameters will not be updated.</p>
<p>💡 The <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> method is used to backpropagate the loss. This step is basically performing the backward pass of the network.</p>
</section>
<section id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">#</a></h2>
<p>Once we have trained our model, we need to evaluate its performance on the test set. The evaluation process is similar to the training process. The main difference is that we do not need to update the parameters of the network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1"># Set the model in evaluation mode</span>
<span class="c1"># Why?</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># Disable gradient computation</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">test_dataloader</span><span class="p">:</span>
        <span class="c1"># Get the input data</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Get the target data</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Forward pass</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="c1"># Loss computation</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case, during evaluation we need to disable the gradient computation. This is necessary because we do not need to update the parameters of the network. If we do not disable the gradient computation, PyTorch will compute the gradients of the network parameters. This is not necessary during evaluation and it is a waste of computational resources.</p>
<p>💡 We did not use any specific metric to evaluate the performance of the model, we only computed the loss. However, in practice, we usually use different metrics to evaluate the performance of the model. For example, in classification tasks, we usually use the accuracy metric. In regression tasks, we usually use the mean squared error metric.
Those metrics can be implemented saving the predictions and the targets and then computing the metric on the whole dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">targets</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># Disable gradient computation</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">test_dataloader</span><span class="p">:</span>
        <span class="c1"># Get the input data</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Get the target data</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Forward pass</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="c1"># Save the predictions</span>
        <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="c1"># Save the targets</span>
        <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

<span class="c1"># Concatenate the predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Concatenate the targets</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Compute the accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="training-my-first-cnn">
<h2>Training My (First) CNN<a class="headerlink" href="#training-my-first-cnn" title="Link to this heading">#</a></h2>
<p>In this section, we will implement a simple CNN for the MNIST dataset. The MNIST dataset is a dataset of handwritten digits. The dataset is composed of 60000 training samples and 10000 test samples. Each sample is a 28x28 grayscale image and the target is a digit between 0 and 9.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span> <span class="c1"># Progress bar</span>

<span class="c1"># Create the dataset</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;data/MNIST/&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;data/MNIST/&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="c1"># we can implement a validation split, how?</span>

<span class="c1"># Create the dataloader</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Define the model</span>
<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">32</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Create the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>

<span class="c1"># Define the loss function</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Define the optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="c1"># Define the scheduler</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># Set the model in training mode</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Iterate over the batches</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="c1"># Get the input data</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Get the target data</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Forward pass</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="c1"># Loss computation</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="c1"># Backward pass</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># Parameters update</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># Reset the gradients</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># Update the learning rate</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Evaluation loop</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1"># Set the model in evaluation mode</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">targets</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># Disable gradient computation</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">test_dataloader</span><span class="p">:</span>
        <span class="c1"># Get the input data</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># Get the target data</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Forward pass</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="c1"># Save the predictions</span>
        <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="c1"># Save the targets</span>
        <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

<span class="c1"># Concatenate the predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Concatenate the targets</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Compute the accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="assignment">
<h3>Assignment<a class="headerlink" href="#assignment" title="Link to this heading">#</a></h3>
<p>Implement a CNN for the ITALIC dataset. The ITALIC dataset is a collection of around 16,000 audio recordings of Italian sentences. The <code class="docutils literal notranslate"><span class="pre">intent</span></code> is a label that describes the purpose of the sentence.</p>
<p>Implement a complete pipeline to train and evaluate a CNN for Intent Classification. The pipeline should include the following steps:</p>
<ul class="simple">
<li><p><strong>Data loading</strong>: load the dataset.</p></li>
<li><p><strong>Data preprocessing</strong>: preprocess the dataset when and if necessary.</p></li>
<li><p><strong>Model definition</strong>: define the model architecture (you can use a 2D CNN if working with spectrograms or a 1D CNN if working with raw audio).</p></li>
<li><p><strong>Model training</strong>: train the model - include a validation split and select the best model based on the validation performance.</p></li>
<li><p><strong>Model evaluation</strong>: evaluate the model - compute specific metrics (e.g., accuracy).</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="1_dl_intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction to Deep Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="3_transformers.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Transformers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Convolutional Neural Networks</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-image-data">Properties of Image Data</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-network-architecture">Convolutional Neural Network Architecture</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-components">Basic Components</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-layers">Convolutional Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-layers">Pooling Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stride-and-padding">Stride and Padding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#receptive-field">Receptive Field</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-cnn-architectures">Common CNN Architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lenet-5">LeNet-5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet">AlexNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet">ResNet</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convnets-for-audio-data">ConvNets for Audio Data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-convolutional-layers">1-D Convolutional Layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-layers-for-temporal-patterns">Convolutional Layers for Temporal Patterns</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectrogram-representation">Spectrogram Representation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#short-time-fourier-transform">Short-Time Fourier Transform</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mel-spectrogram">Mel-Spectrogram</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#audio-cnn-architectures">Audio CNN Architectures</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-a-cnn-with-pytorch">Implementing a CNN with PyTorch</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">MNIST</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#italian-intent-classification-italic">ITALian Intent Classification (ITALIC)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataloader">DataLoader</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Convolutional Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizers-and-schedulers">Optimizers and Schedulers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-my-first-cnn">Training My (First) CNN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment">Assignment</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Moreno La Quatra
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>