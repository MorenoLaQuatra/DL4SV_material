
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Transformers &#8212; Deep Learning for Speech and Vision</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js?v=afe5de03"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '3_transformers';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Deep Learning Libraries" href="4_libraries.html" />
    <link rel="prev" title="Convolutional Neural Networks" href="2_cnns.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/cover_2.png" class="logo__image only-light" alt="Deep Learning for Speech and Vision - Home"/>
    <script>document.write(`<img src="_static/cover_2.png" class="logo__image only-dark" alt="Deep Learning for Speech and Vision - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Deep Learning for Speech and Vision
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_dl_intro.html">Introduction to Deep Learning</a></li>


<li class="toctree-l1"><a class="reference internal" href="2_cnns.html">Convolutional Neural Networks</a></li>




<li class="toctree-l1 current active"><a class="current reference internal" href="#">Transformers</a></li>



<li class="toctree-l1"><a class="reference internal" href="4_libraries.html">Deep Learning Libraries</a></li>


<li class="toctree-l1"><a class="reference internal" href="5_applications.html">Applications and final projects</a></li>



<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F3_transformers.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/3_transformers.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Transformers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Transformers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-overview">Architecture Overview</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-decoder">Encoder-Decoder</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-components">Transformer Components</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-layer">Embedding Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanism">Attention Mechanism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-and-residual-connections">Feed-Forward and Residual Connections</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-and-decoder-models">Encoder and Decoder Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-decoder-sequence-to-sequence-models">Encoder-Decoder (Sequence-to-sequence) Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-encoder-decoder-model-in-pytorch">An Encoder-Decoder Model in PyTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Embedding Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Positional Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Attention Mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention">Cross-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Feed-Forward and Residual Connections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Encoder and Decoder Models</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="transformers">
<h1>Transformers<a class="headerlink" href="#transformers" title="Link to this heading">#</a></h1>
<figure class="align-default" id="cover">
<a class="reference internal image-reference" href="_images/cover_transformers.png"><img alt="cover" src="_images/cover_transformers.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 33 </span><span class="caption-text">Image generated using <a class="reference external" href="https://huggingface.co/spaces/mrfakename/OpenDalleV1.1-GPU-Demo">OpenDALL-E</a></span><a class="headerlink" href="#cover" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In this chapter, we will cover the basics of transformers, a type of neural network architecture that has been initially developed for natural language processing (NLP) tasks but has since been used and adapted for other modalities such as images, audio, and video.</p>
<p>The transformer architecture is designed for modeling sequential data, such as text, audio, and video. It is based on the idea of self-attention, which is a mechanism that allows the network to learn the relationships between different elements of a sequence. For example, in the case of an audio sequence, the network can learn the relationships between different frames of the audio signal and leverage the correlations between them to perform a task such as speech recognition.</p>
<p>The original Transformer architecture was introduced in the paper <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> <span id="id1">[<a class="reference internal" href="references.html#id13" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.">VSP+17</a>]</span>
by Vaswani et al. in 2017. Since then, many variants of the original architecture have been proposed, and transformers have become the state-of-the-art architecture for many tasks. In this chapter, we will cover all the building blocks of the transformer architecture and show how they can be used for different tasks.</p>
<figure class="align-default" id="architecture">
<a class="reference internal image-reference" href="_images/architecture.webp"><img alt="architecture" src="_images/architecture.webp" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 34 </span><span class="caption-text">Transformer architecture</span><a class="headerlink" href="#architecture" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="architecture-overview">
<h1>Architecture Overview<a class="headerlink" href="#architecture-overview" title="Link to this heading">#</a></h1>
<p><a class="reference internal" href="#architecture"><span class="std std-numref">Fig. 34</span></a> shows the architecture of the transformer model. The model consists of an encoder and a decoder.</p>
<ul class="simple">
<li><p>The <strong>encoder</strong> is responsible for processing the input sequence and extracting relevant information.</p></li>
<li><p>The <strong>decoder</strong> is responsible for generating the output sequence based on the information extracted by the encoder.</p></li>
</ul>
<p>The encoder and decoder are composed of a stack of identical layers. Each layer consists of multiple components, including a multi-head self-attention mechanism and a feed-forward network. We will cover each of these components in detail in the following sections.</p>
<p>💡 Transformers are designed to model <strong>discrete</strong> sequences, such as words in text, genes in DNA, or tokens in a programming language. They are not designed to model <strong>continuous</strong> sequences, such as audio or video. However, transformers can be used to model continuous sequences by discretizing them with specific techniques.</p>
<p>A few concepts are important to understand before we dive into the details of the transformer architecture.</p>
<ul class="simple">
<li><p><strong>Pre-training and fine-tuning</strong> is a technique that consists of training a model on a large amount of unlabeled data. The model is then fine-tuned on a specific task using a small amount of labeled data. Pre-training is a common technique used in deep learning to improve the performance of a model on a specific task. BERT <span id="id2">[<a class="reference internal" href="references.html#id14" title="Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.">DCLT18</a>]</span>, Wav2Vec 2.0 <span id="id3">[<a class="reference internal" href="references.html#id15" title="Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: a framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449–12460, 2020.">BZMA20</a>]</span>, and ViT <span id="id4">[<a class="reference internal" href="references.html#id16" title="Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others. An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.">DBK+20</a>]</span> are examples of models that have been pre-trained on large amounts of data and fine-tuned on specific tasks.</p></li>
<li><p><strong>Discretization</strong> is used to convert continuous sequences into discrete sequences. For example, an audio signal is a continuous sequence, to convert it into a discrete sequence, we can split it into frames and define a <em>vocabulary</em> of frames. Once discretized, we can use <em>classification-like</em> approaches to train a transformer model on the audio sequence. Wav2Vec 2.0 <span id="id5">[<a class="reference internal" href="references.html#id15" title="Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. Wav2vec 2.0: a framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449–12460, 2020.">BZMA20</a>]</span> is an example of a model that uses discretization to train a transformer model on audio sequences.</p></li>
<li><p><strong>Positional encoding</strong> is a technique that consists of injecting information about the position of each element of a sequence into the model. We will see later that <em>attention</em> is a mechanism that allows the model to learn the relationships between the different elements of a sequence but it does not take into account the position of each element. Positional encoding is used to inject this information into the model.</p></li>
<li><p><strong>Encoder models</strong> are transformer models that only have an encoder. They are used to extract features from a sequence. BERT <span id="id6">[<a class="reference internal" href="references.html#id14" title="Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.">DCLT18</a>]</span> and ViT <span id="id7">[<a class="reference internal" href="references.html#id16" title="Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others. An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.">DBK+20</a>]</span> are examples of encoder models.</p></li>
<li><p><strong>Decoder models</strong> are transformer models that only have a decoder. They are used to generate a sequence based on a set of features. GPT-2 <span id="id8">[<a class="reference internal" href="references.html#id20" title="Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and others. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.">RWC+19</a>]</span> and VioLA <span id="id9">[<a class="reference internal" href="references.html#id17" title="Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu Wei. Viola: unified codec language models for speech recognition, synthesis, and translation. arXiv preprint arXiv:2305.16107, 2023.">WZZ+23</a>]</span> are examples of decoder models.</p></li>
<li><p><strong>Sequence-to-sequence models</strong> are transformer models that have both an encoder and a decoder. They are used to generate a sequence based on another sequence. BART <span id="id10">[]</span> and Whisper <span id="id11">[<a class="reference internal" href="references.html#id18" title="Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, 28492–28518. PMLR, 2023.">RKX+23</a>]</span> are examples of sequence-to-sequence models.</p></li>
</ul>
<p>Those concepts will be used throughout this chapter to describe the different transformer models.</p>
<section id="encoder">
<h2>Encoder<a class="headerlink" href="#encoder" title="Link to this heading">#</a></h2>
<p>The encoder is responsible for processing the input sequence and extracting relevant information. The goal is to train a neural network that can leverage the correlations between the different elements of the input sequence to perform <strong>discriminative</strong> tasks such as classification, regression, or sequence labeling.</p>
<p>The input of the encoder is a sequence of elements. For example, in the case of text, the input sequence is a sequence of words. In the case of audio, the input sequence is a sequence of frames. In the case of images, the input sequence is a sequence of patches. The sequence is first converted into a sequence of <em>vector embeddings</em> that are then processed by the encoder layers.</p>
<figure class="align-default" id="id12">
<a class="reference internal image-reference" href="_images/encoder_with_tensors_2.png"><img alt="encoder" src="_images/encoder_with_tensors_2.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 35 </span><span class="caption-text">Encoder Layer architecture. Image source <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a></span><a class="headerlink" href="#id12" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#id12"><span class="std std-numref">Fig. 35</span></a> shows the architecture of an encoder layer. The input sequence is first converted into a sequence of vector embeddings <span class="math notranslate nohighlight">\(X = \{x_1, x_2, ..., x_n\}\)</span> using an embedding layer. The embeddings are then processed by the self-attention layer and then pass through a feed-forward network. The output of the feed-forward network is then added to the input embeddings to produce the output embeddings <span class="math notranslate nohighlight">\(Y = \{y_1, y_2, ..., y_n\}\)</span>.</p>
<p>The encoder is composed of a stack of identical layers, all similar to the one shown in <a class="reference internal" href="#id12"><span class="std std-numref">Fig. 35</span></a>. The output of the encoder is the output embeddings <span class="math notranslate nohighlight">\(Y\)</span> of the last layer.</p>
</section>
<section id="decoder">
<h2>Decoder<a class="headerlink" href="#decoder" title="Link to this heading">#</a></h2>
<p>The decoder is responsible for generating the output sequence based on the information extracted by the encoder. The goal is to train a neural network that can leverage the correlations between the different elements of the input sequence to perform <strong>generative</strong> tasks such as text generation, image generation, or speech synthesis.</p>
<p>The input of the decoder is a sequence of elements. For example, in the case of audio, the input sequence is a sequence of frames. The sequence is first converted into a sequence of <em>vector embeddings</em> that are then processed by the decoder layers.</p>
<figure class="align-default" id="id13">
<a class="reference internal image-reference" href="_images/transformer-decoder-intro.png"><img alt="decoder" src="_images/transformer-decoder-intro.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 36 </span><span class="caption-text">Decoder Layer architecture. Image source <a class="reference external" href="https://jalammar.github.io/illustrated-gpt2/">illustrated-gpt-2</a></span><a class="headerlink" href="#id13" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The <em>masked self-attention</em> layer is similar to the self-attention layer of the encoder. The only difference is that the masked self-attention layer is masked to prevent the decoder from “seeing” the future elements of the sequence. The output is then processed by a feed-forward network. The output of the feed-forward network is then added to the input embeddings to produce the output embeddings <span class="math notranslate nohighlight">\(Y = \{y_1, y_2, ..., y_n\}\)</span>.</p>
<p>When training a decoder-only model, the output embeddings <span class="math notranslate nohighlight">\(Y\)</span> at each position <span class="math notranslate nohighlight">\(i\)</span> are used to predict the next element of the sequence <span class="math notranslate nohighlight">\(y_{i+1}\)</span>.</p>
<p>💡 In contrast with RNNs, the training of the decoder is <strong>autoregressive</strong>. This means that the model is trained to predict the next element of the sequence based on the previous elements of the sequence. While for RNNs we need to recursively feed the output of the model back as input, for transformers we can compute the output of the model in parallel for all the elements of the sequence.</p>
<p>💡 During <strong>inference</strong>, the decoder is used to generate the output sequence. However, the target is not available during inference. Instead, the output of the decoder at each position <span class="math notranslate nohighlight">\(i\)</span> is used as input for the next position <span class="math notranslate nohighlight">\(i+1\)</span>. This process is repeated until a special token is generated or a maximum number of steps is reached. This is one of the reason why, the <strong>inference</strong> on transformers is slower than the <strong>training</strong>.</p>
</section>
<section id="encoder-decoder">
<h2>Encoder-Decoder<a class="headerlink" href="#encoder-decoder" title="Link to this heading">#</a></h2>
<p>If we combine the encoder and decoder, we get a sequence-to-sequence model. The encoder is used to extract features from the input sequence and the decoder is used to generate the output sequence based (or conditioned) on the extracted features.
One example of sequence-to-sequence model is a music style transfer model. The input sequence may be a song in a specific style and the output sequence may be the same song in another style. The encoder is used to extract features from the input song and the decoder is used to generate the output song based on the extracted features.</p>
<figure class="align-default" id="id14">
<a class="reference internal image-reference" href="_images/encoder_decoder.png"><img alt="encoder_decoder" src="_images/encoder_decoder.png" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 37 </span><span class="caption-text">Encoder-Decoder architecture. Image source <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a></span><a class="headerlink" href="#id14" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#id14"><span class="std std-numref">Fig. 37</span></a> shows the architecture of an encoder-decoder model. The input sequence is first converted into a sequence of vector embeddings <span class="math notranslate nohighlight">\(X = \{x_1, x_2, ..., x_n\}\)</span> using an embedding layer. The embeddings are then processed by the encoder layers. The output of the encoder is the output embeddings <span class="math notranslate nohighlight">\(Y = \{y_1, y_2, ..., y_n\}\)</span> of the last layer. The output embeddings are then processed by the decoder layers. Here we have an <strong>additional</strong> attention layer that allows the decoder to combine the output embeddings of the encoder with the output embeddings of the decoder. The output of the decoder is the output embeddings <span class="math notranslate nohighlight">\(Z = \{z_1, z_2, ..., z_n\}\)</span> of the last layer.</p>
<p>🖊️ The encoder-decoder attention is usually referred to as the <strong>cross-attention</strong> layer. The self-attention layer in the encoder is usually referred to as the <strong>self-attention</strong> layer. The self-attention layer in the decoder is usually referred to as the <strong>masked self-attention</strong> layer because it is masked to prevent the decoder from “seeing” the future elements of the sequence. All these layers, however, perform the same operation that we will describe in the following sections.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="transformer-components">
<h1>Transformer Components<a class="headerlink" href="#transformer-components" title="Link to this heading">#</a></h1>
<figure class="align-default" id="architecture-2">
<a class="reference internal image-reference" href="_images/architecture.webp"><img alt="architecture_2" src="_images/architecture.webp" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 38 </span><span class="caption-text">Encoder-decoder Transformer architecture.</span><a class="headerlink" href="#architecture-2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>We will describe the different components of the transformer architecture from <strong>bottom to top</strong>. We will follow the <a class="reference internal" href="#architecture"><span class="std std-numref">Fig. 34</span></a> and start with the embedding layer, then the positional encoding, the self-attention layer, and so on.</p>
<section id="embedding-layer">
<h2>Embedding Layer<a class="headerlink" href="#embedding-layer" title="Link to this heading">#</a></h2>
<p>When training a transformer model, the input sequence is first converted into a sequence of <em>vector embeddings</em>. Those vector embeddings are created using an embedding layer. The embedding layer is a simple linear layer that maps each element of the input sequence to a vector of a specific size. The size of the vector is called the <em>embedding size</em> and is a power of 2, usually between 128 and 1024. The embedding size is a <strong>hyperparameter</strong> of the model.</p>
<p>We can see the embedding layer as a lookup table that maps each element of the input sequence to a vector of a specific size. The embedding layer is initialized randomly and is trained through backpropagation. The embedding layer is usually the first layer of the encoder and the decoder.</p>
<figure class="align-default" id="lookup-table">
<a class="reference internal image-reference" href="_images/lookup_table.gif"><img alt="lookup_table" src="_images/lookup_table.gif" style="width: 70%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 39 </span><span class="caption-text">Embedding layer as a lookup table. Image source <a class="reference external" href="https://lena-voita.github.io/nlp_course/word_embeddings.html">lena-voita</a></span><a class="headerlink" href="#lookup-table" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#lookup-table"><span class="std std-numref">Fig. 39</span></a> shows an example of an embedding layer in the context of NLP (it is simpler to visualize in this context). The embedding layer is a lookup table that maps each word of the input sequence to a vector of a specific size.</p>
</section>
<section id="positional-encoding">
<span id="pos-encoding-section"></span><h2>Positional Encoding<a class="headerlink" href="#positional-encoding" title="Link to this heading">#</a></h2>
<p>After the embedding layer, the input sequence is converted into a sequence of vector embeddings. As we can see later, the attention mechanism, at the core of the transformer architecture, does not take into account the position of each element of the sequence. To inject this information into the model, we use a technique called <em>positional encoding</em>.</p>
<p>There are different implementations of positional encoding. The traditional implementation is based on sinusoidal functions. For each position <span class="math notranslate nohighlight">\(i\)</span> of the input sequence, we compute a vector <span class="math notranslate nohighlight">\(PE_i\)</span> of the same size as the embeddings. The vector <span class="math notranslate nohighlight">\(PE_i\)</span> is then added to the embeddings <span class="math notranslate nohighlight">\(x_i\)</span> to produce the final embeddings <span class="math notranslate nohighlight">\(x_i + PE_i\)</span>.</p>
<p><strong>How can we compute the vector <span class="math notranslate nohighlight">\(PE_i\)</span>?</strong> The vector <span class="math notranslate nohighlight">\(PE_i\)</span> is computed using a combination of sinusoidal functions. We define a set of frequencies <span class="math notranslate nohighlight">\(f\)</span> and compute the vector <span class="math notranslate nohighlight">\(PE_i\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}PE_i = \begin{bmatrix} sin(f_1 \times i) \\ cos(f_1 \times i) \\ sin(f_2 \times i) \\ cos(f_2 \times i) \\ \vdots \\ sin(f_{d/2} \times i) \\ cos(f_{d/2} \times i) \end{bmatrix}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the size of the embeddings. The frequencies <span class="math notranslate nohighlight">\(f\)</span> are computed as follows:</p>
<div class="math notranslate nohighlight">
\[f_i = \frac{1}{10000^{2i/d}}\]</div>
<p>The frequencies <span class="math notranslate nohighlight">\(f\)</span> are computed using a geometric progression. The first frequency is <span class="math notranslate nohighlight">\(f_1 = 1/10000^{2 \times 1/d}\)</span>, the second frequency is <span class="math notranslate nohighlight">\(f_2 = 1/10000^{2 \times 2/d}\)</span>, and so on. The frequencies are then used to compute the vector <span class="math notranslate nohighlight">\(PE_i\)</span>. <span class="math notranslate nohighlight">\(d\)</span> is the size of the embeddings. The vector <span class="math notranslate nohighlight">\(PE_i\)</span> is then added to the embeddings <span class="math notranslate nohighlight">\(x_i\)</span> to produce the vector <span class="math notranslate nohighlight">\(x_i + PE_i\)</span> that will be the input of the network.</p>
<figure class="align-default" id="id15">
<a class="reference internal image-reference" href="_images/transformer_positional_encoding_example.png"><img alt="positional_encoding" src="_images/transformer_positional_encoding_example.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 40 </span><span class="caption-text">Positional encoding example. Image source <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a></span><a class="headerlink" href="#id15" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#id15"><span class="std std-numref">Fig. 40</span></a> shows an example of positional encoding (again in the context of NLP). Each element in a <span class="math notranslate nohighlight">\(PE_i\)</span> vector is computed using a sinusoidal function.</p>
<p>💡 The idea behind the use of sinusoidal functions is to allow the model to be able to encode regularities in the position of the elements of the sequence. Different frequencies may have similar values with different regularities. In a complete data-driven approach, the model would learn the regularities according to the patterns found in the data.</p>
</section>
<section id="attention-mechanism">
<h2>Attention Mechanism<a class="headerlink" href="#attention-mechanism" title="Link to this heading">#</a></h2>
<p>At this point of the chapter, we have converted the input sequence into a sequence of vector embeddings. The next step is to process the embeddings using the attention mechanism. The attention mechanism is the core of the transformer architecture. It is used to learn the relationships between the different elements of the sequence.</p>
<p>The attention mechanism is a mechanism that allows the model to learn the relationships between the different elements of the sequence. the process can be divided into three steps:</p>
<ol class="arabic simple">
<li><p><strong>Query, Key, and Value</strong>. The input embeddings are first <em>split</em> into three vectors: the query vector, the key vector, and the value vector.</p></li>
<li><p><strong>Attention</strong>. The query vector is compared to the key vector to produce a score, e.g., a float value between 0 and 1. The score is then used to compute a weighted average of the value vector. The weighted average is called the <em>attention vector</em>.</p></li>
<li><p><strong>Output</strong>. The attention vector is then processed by a linear layer to produce the output vector.</p></li>
</ol>
<figure class="align-default" id="attention-animation">
<a class="reference internal image-reference" href="_images/attention.gif"><img alt="attention_animation" src="_images/attention.gif" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 41 </span><span class="caption-text">Attention mechanism steps. Image source <a class="reference external" href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b2">towardsdatascience</a></span><a class="headerlink" href="#attention-animation" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#attention-animation"><span class="std std-numref">Fig. 41</span></a> shows an example of the attention mechanism. The input embeddings are first split into three vectors: the query vector, the key vector, and the value vector.</p>
<ul class="simple">
<li><p><strong>Query</strong>. The query vector is used to <em>ask</em> to all other elements of the sequence <em>how much</em> they are related to the current element. The dot product between the query vector and the key vector is used to compute a score (the higher the score, the more related the elements are). The score is then normalized using a softmax function to produce a probability distribution over all the elements of the sequence.</p></li>
<li><p><strong>Key</strong>. The key vector is used to <em>answer</em> to the query. Each value vector is multiplied with the <em>query</em> of the other elements of the sequence. <em>query</em> and <em>key</em> are the vectors that, multiplied together, produce the score.</p></li>
<li><p><strong>Value</strong>. Once obtained a score for each element of the sequence, the <em>value</em> vector is multiplied with the score to produce a weighted average of the value vector. The weighted average is called the <em>attention vector</em>.</p></li>
</ul>
<p>The final vector representation for a given input element of the sequence is given by the sum of the attention vectors of all the elements of the sequence.</p>
<p>💡 Note that, the attention mechanism is usually referred to <em>self</em>-attention because the attention score is computed between a given element of the sequence and all the other elements, including itself.</p>
<p>If we put this into equations, we have:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\]</div>
<p>where <span class="math notranslate nohighlight">\(Q\)</span> is the query vector, <span class="math notranslate nohighlight">\(K\)</span> is the key vector, <span class="math notranslate nohighlight">\(V\)</span> is the value vector, and <span class="math notranslate nohighlight">\(d_k\)</span> is the size of the key vector. <span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span> is used to scale the dot product between <span class="math notranslate nohighlight">\(Q\)</span> and <span class="math notranslate nohighlight">\(K\)</span>. The softmax function is applied to each row of the matrix <span class="math notranslate nohighlight">\(QK^T\)</span> to produce a probability distribution over all the elements of the sequence. The probability distribution is then used to compute a weighted average of the value vector <span class="math notranslate nohighlight">\(V\)</span>.
The formula above is the <em>matrix form</em> of the attention mechanism.</p>
<p>Each element of the sequence is processed independently by the attention mechanism. This means that the attention mechanism can be computed in parallel for all the elements of the sequence. This is one of the reasons why transformers are faster than RNNs on modern hardware (e.g., GPUs).</p>
<p>Coming back to our bottom-up approach, the attention mechanism is used to process the embeddings of the input sequence (after the positional encoding). The output of the attention mechanism is a sequence of vectors having the same size and shape as the input embeddings. After the attention mechanism a simple linear layer is used to produce the output embeddings (typically without altering the size of the embeddings).</p>
<p><strong>Multi-head Attention</strong>. The attention mechanism described above is called <em>single-head attention</em>. In practice, the attention mechanism is computed multiple times in parallel on subsets of the embeddings. Each subset is called a <em>head</em>. Before feeding the embedding into the self-attention layer, in case of multi-head attention, the vector is first split into parts and each part is processed by a different head. The output of the self-attention layer is then the concatenation of the output of each head. The output of the self-attention layer is then processed by a linear layer to produce the output embeddings.</p>
<figure class="align-default" id="multi-head-attention">
<a class="reference internal image-reference" href="_images/multi-head-attention.svg"><img alt="multi-head-attention" src="_images/multi-head-attention.svg" width="50%" /></a>
<figcaption>
<p><span class="caption-number">Fig. 42 </span><span class="caption-text">Example of multi-head attention (e.g., number of attention heads <span class="math notranslate nohighlight">\(h=2\)</span>).</span><a class="headerlink" href="#multi-head-attention" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#multi-head-attention"><span class="std std-numref">Fig. 42</span></a> shows an example of multi-head attention. In practice, all implementations of modern transformer models use multi-head attention (e.g., BERT, GPT-2, ViT, etc.). The number of heads is a <strong>hyperparameter</strong> of the model, similarly to the embedding size. It is worth noting that, the number of heads should be a number such that the size of the input embedding is divisible by the number of heads.
For example, if the embedding size is <span class="math notranslate nohighlight">\(512\)</span>, we can use <span class="math notranslate nohighlight">\(8\)</span> heads (<span class="math notranslate nohighlight">\(512/8 = 64\)</span>) or <span class="math notranslate nohighlight">\(16\)</span> heads (<span class="math notranslate nohighlight">\(512/16 = 32\)</span>) but not <span class="math notranslate nohighlight">\(10\)</span> heads (<span class="math notranslate nohighlight">\(512/10 = 51.2\)</span>).</p>
</section>
<section id="feed-forward-and-residual-connections">
<h2>Feed-Forward and Residual Connections<a class="headerlink" href="#feed-forward-and-residual-connections" title="Link to this heading">#</a></h2>
<p>After the attention mechanism, the output embeddings are processed by a feed-forward network. The feed-forward network is a simple linear layer followed by a non-linear activation function (e.g., ReLU).</p>
<p>Similarly to what we have seen with ResNets <span id="id16">[]</span>, in each layer of the encoder and decoder, there are <em>residual connections</em> that sum up the output of a sub-layer with the input of the sub-layer.</p>
<figure class="align-default" id="transformer-residual-layer-norm">
<a class="reference internal image-reference" href="_images/transformer_residual_layer_norm_2.png"><img alt="transformer_residual_layer_norm" src="_images/transformer_residual_layer_norm_2.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 43 </span><span class="caption-text">Residual connections and layer normalization. Image source <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a></span><a class="headerlink" href="#transformer-residual-layer-norm" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#transformer-residual-layer-norm"><span class="std std-numref">Fig. 43</span></a> shows an example of residual connections and layer normalization. Both the output of the attention layer and the output of the feed-forward network are added to the input embeddings. The output of the residual connections is then processed by a layer normalization layer. Layer normalization is a technique that consists of normalizing the output of a layer using the mean and variance of the output of the layer. Layer normalization is used to make the training of the model more stable and efficient.</p>
</section>
<section id="encoder-and-decoder-models">
<h2>Encoder and Decoder Models<a class="headerlink" href="#encoder-and-decoder-models" title="Link to this heading">#</a></h2>
<p>At this point we have all the ingredients to create an encoder transformer layer. The encoder model is composed of:</p>
<ul class="simple">
<li><p><strong>Embedding layer</strong>. The embedding layer converts the input sequence into a sequence of vector embeddings.</p></li>
<li><p><strong>Positional encoding</strong>. The positional encoding injects information about the position of each element of the sequence into the model.</p></li>
<li><p><strong>Encoder layers</strong>. The encoder layers process the embeddings using:</p>
<ul>
<li><p><strong>Multi-head attention</strong>. The multi-head attention mechanism is used to learn the relationships between the different elements of the sequence.</p></li>
<li><p><strong>Feed-forward network</strong>. The feed-forward network is used to process the output of the multi-head attention mechanism.</p></li>
<li><p><strong>Residual connections</strong>. The residual connections are used to sum up the output of the multi-head attention mechanism with the input embeddings.</p></li>
<li><p><strong>Layer normalization</strong>. The layer normalization is used to normalize the output of the encoder layer.</p></li>
</ul>
</li>
</ul>
<p>A stack of encoder layers is used to create the encoder model. The output of the encoder is the output embeddings of the last encoder layer.
We can use this encoder model to extract features from a sequence. For example, we can use the encoder model to extract features from an audio sequence and then use those features to perform speech recognition.</p>
<p>The decoder model, when used in decoder-only mode, is similar to the encoder model. The decoder model is composed of:</p>
<ul class="simple">
<li><p><strong>Embedding layer</strong>. The embedding layer converts the input sequence into a sequence of vector embeddings.</p></li>
<li><p><strong>Positional encoding</strong>. The positional encoding injects information about the position of each element of the sequence into the model.</p></li>
<li><p><strong>Decoder layers</strong>. The decoder layers process the embeddings using:</p>
<ul>
<li><p><strong>Masked multi-head attention</strong>. The masked multi-head attention mechanism is used to learn the relationships between the different elements of the sequence. The attention mechanism is masked to prevent the decoder from “seeing” the future elements of the sequence.</p></li>
<li><p><strong>Feed-forward network</strong>. The feed-forward network is used to process the output of the multi-head attention mechanism.</p></li>
<li><p><strong>Residual connections</strong>. The residual connections are used to sum up the output of the multi-head attention mechanism with the input embeddings.</p></li>
<li><p><strong>Layer normalization</strong>. The layer normalization is used to normalize the output of the decoder layer.</p></li>
</ul>
</li>
</ul>
<p>Notice that, the attention layer of the decoder is referred as <strong>masked</strong> multi-head attention because it is masked to prevent the decoder from “seeing” the future elements of the sequence.</p>
<figure class="align-default" id="masked-self-attention">
<a class="reference internal image-reference" href="_images/masked-self-attention.png"><img alt="masked-self-attention" src="_images/masked-self-attention.png" style="width: 75%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 44 </span><span class="caption-text">Comparison between self-attention and masked self-attention. Image source <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a></span><a class="headerlink" href="#masked-self-attention" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#masked-self-attention"><span class="std std-numref">Fig. 44</span></a> shows an example of masked self-attention and the comparison with self-attention operation. When processing element <span class="math notranslate nohighlight">\(x_i\)</span>, the masked self-attention mechanism is masked to prevent the decoder from “seeing” the future elements of the sequence <span class="math notranslate nohighlight">\(x_{i+1}, x_{i+2}, ..., x_n\)</span>. The <em>masked</em>-self attention mechanism is usually implemented by adding a mask to the softmax function. The mask contains <span class="math notranslate nohighlight">\(-\infty\)</span> values for all the elements of the sequence that we want to mask. The <span class="math notranslate nohighlight">\(-\infty\)</span> values are used to set the attention score to <span class="math notranslate nohighlight">\(0\)</span> after the softmax function. This means that the decoder will not be able to attend to the masked elements of the sequence.</p>
</section>
<section id="encoder-decoder-sequence-to-sequence-models">
<h2>Encoder-Decoder (Sequence-to-sequence) Models<a class="headerlink" href="#encoder-decoder-sequence-to-sequence-models" title="Link to this heading">#</a></h2>
<p>We have seen how to design layers for the encoder and decoder models. <strong>Encoder</strong> models are used to extract features from a sequence. <strong>Decoder</strong> models are used to generate a sequence <em>based on</em> the previous elements of the sequence. <strong>Encoder-decoder</strong> models are used to generate data <em>conditioned on</em> another sequence. For example, we can use an encoder-decoder model to translate a sentence from English to French or to generate the transcription of an audio sequence.</p>
<p>The encoder-decoder model is composed of:</p>
<ul class="simple">
<li><p><strong>Encoder</strong>. The encoder is used to extract features from the input sequence.</p></li>
<li><p><strong>Decoder</strong>. The decoder is used to generate the output sequence based on the extracted features.</p>
<ul>
<li><p><strong>Cross-attention (encoder-decoder attention)</strong>. The decoder in this case adds an additional attention layer that allows the decoder to <em>condition</em> the output sequence on the input sequence. The cross-attention layer is similar to the self-attention layer of the encoder. The only difference is that the cross-attention layer is used to learn the relationships between the elements of the input sequence and the elements of the output sequence.</p></li>
</ul>
</li>
</ul>
<figure class="align-default" id="cross-attention-endec">
<a class="reference internal image-reference" href="_images/cross-attention-endec.png"><img alt="cross-attention-endec" src="_images/cross-attention-endec.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 45 </span><span class="caption-text">Encoder-decoder model showing the cross-attention layer. Image source <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">illustrated-transformer</a></span><a class="headerlink" href="#cross-attention-endec" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#cross-attention-endec"><span class="std std-numref">Fig. 45</span></a> shows an example of an encoder-decoder model. The input sequence is first converted into a sequence of vector embeddings <span class="math notranslate nohighlight">\(X = \{x_1, x_2, ..., x_n\}\)</span> using an embedding layer. The embeddings are then processed by the encoder layers. The output of the encoder is the output embeddings <span class="math notranslate nohighlight">\(Y = \{y_1, y_2, ..., y_n\}\)</span> of the last layer. The output embeddings are then processed by the decoder layers. Here we have an <strong>additional</strong> attention layer that leverage <strong>key</strong> and <strong>value</strong> vectors from the encoder to combine the output embeddings of the encoder with the output embeddings of the decoder. The output of the decoder is the output embeddings <span class="math notranslate nohighlight">\(Z = \{z_1, z_2, ..., z_n\}\)</span> of the last layer.
The example reported in <a class="reference internal" href="#cross-attention-endec"><span class="std std-numref">Fig. 45</span></a> is in the context of NLP. However, the very same architecture can be used in the context of audio, images, or multi-modal data (e.g., an audio sequence is encoded into a sequence of embeddings and then decoded into a sequence of text for speech recognition) <span id="id17">[<a class="reference internal" href="references.html#id18" title="Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, 28492–28518. PMLR, 2023.">RKX+23</a>]</span>.</p>
</section>
<section id="an-encoder-decoder-model-in-pytorch">
<h2>An Encoder-Decoder Model in PyTorch<a class="headerlink" href="#an-encoder-decoder-model-in-pytorch" title="Link to this heading">#</a></h2>
<p>We have seen how to design the different components of the transformer architecture. In this section, we will see how to implement an encoder-decoder model in pure PyTorch.</p>
<section id="id18">
<h3>Embedding Layer<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<p>The embedding layer is a simple linear layer that maps each element of the input sequence to a vector of a specific size. The size of the vector is called the <em>embedding size</em> and is (for convenience) a power of 2, or at least divisible by 2 and 3.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">EmbeddingLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>When calling the embedding layer, we pass the input sequence in terms of <strong>ids</strong>. The ids are integers that represent the elements of the sequence. For example, in the case of text, the ids are the indices of the words in the vocabulary. In the case of audio, the ids are the indices of the frames in the vocabulary. Depending on the domain, we may not need an embedding layer. For example, in the case of images, we can use the pixel values as input to the model.</p>
<p>Let’s take a look at the input-output shape of the embedding layer.</p>
<ul class="simple">
<li><p><strong>Input</strong>. The input of the embedding layer is a sequence of ids. The shape of the input is <span class="math notranslate nohighlight">\((B, S)\)</span> where <span class="math notranslate nohighlight">\(B\)</span> is the batch size and <span class="math notranslate nohighlight">\(S\)</span> is the sequence length.</p></li>
<li><p><strong>Output</strong>. The output of the embedding layer is a sequence of vector embeddings. The shape of the output is <span class="math notranslate nohighlight">\((B, S, E)\)</span> where <span class="math notranslate nohighlight">\(B\)</span> is the batch size, <span class="math notranslate nohighlight">\(S\)</span> is the sequence length, and <span class="math notranslate nohighlight">\(E\)</span> is the embedding size.</p></li>
</ul>
</section>
<section id="id19">
<h3>Positional Encoding<a class="headerlink" href="#id19" title="Link to this heading">#</a></h3>
<p>The positional encoding is a technique that consists of <strong>injecting information about the position</strong> of each element of the sequence into the model. There are different implementations of positional encoding. The traditional implementation is based on sinusoidal functions. For each position <span class="math notranslate nohighlight">\(i\)</span> of the input sequence, we compute a vector <span class="math notranslate nohighlight">\(PE_i\)</span> of the same size as the embeddings. The vector <span class="math notranslate nohighlight">\(PE_i\)</span> is then added to the embeddings <span class="math notranslate nohighlight">\(x_i\)</span> to produce the final embeddings <span class="math notranslate nohighlight">\(x_i + PE_i\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">embedding_size</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pe&#39;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">:]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The positional encoding is implemented as a PyTorch module. It is initialized with the embedding size and the dropout rate. The positional encoding is computed in the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method. We define a set of frequencies <span class="math notranslate nohighlight">\(f\)</span> and compute the vector <span class="math notranslate nohighlight">\(PE_i\)</span> accordingly (review the Section on <a class="reference internal" href="#pos-encoding-section"><span class="std std-ref">Positional Encoding</span></a> for more details).</p>
<p>When passing through the positional encoding layer, the input embeddings are summed up with the positional encoding vectors, therefore their shape does not change $<span class="math notranslate nohighlight">\((B, S, E) \rightarrow (B, S, E)\)</span>$.</p>
<p>There are several other implementations of positional encoding. For example, we can have a learnable positional encoding that is learned during training. It is implemented using an embedding layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">position</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">position</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case, the positional encoding is learned during training. We should pay attention to the fact that this implementation does not force the model to learn the time-related patterns in the data. The name <em>positional encoding</em> is only used for convenience as it may learn <strong>other</strong> patterns in the data.</p>
</section>
<section id="id20">
<h3>Attention Mechanism<a class="headerlink" href="#id20" title="Link to this heading">#</a></h3>
<p>Diving inside the transformer architecture, we need to implement the attention mechanism. The attention mechanism is a mechanism that allows the model to learn the relationships between the different elements of the sequence. The process can be divided into three steps:</p>
<ol class="arabic simple">
<li><p><strong>Query, Key, and Value</strong>. The input embeddings are first <em>split</em> into three vectors: the query vector, the key vector, and the value vector.</p></li>
<li><p><strong>Attention</strong>. The query vector is compared to the key vector to produce a score, e.g., a float value between 0 and 1. The score is then used to compute a weighted average of the value vector. The weighted average is called the <em>attention vector</em>.</p></li>
<li><p><strong>Output</strong>. The attention vector is then processed by a linear layer to produce the output vector.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">MHSA</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">=</span> <span class="n">embedding_size</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">E</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">E</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
<p>The class <code class="docutils literal notranslate"><span class="pre">MHSA</span></code> implements the multi-head attention mechanism. The input of the attention mechanism is a sequence of embeddings. The embeddings are first split into three vectors: the query vector, the key vector, and the value vector. The query, key, and value vectors are then processed by three linear layers. The output of the linear layers is then split into multiple heads. The number of heads is a <strong>hyperparameter</strong> of the model. The output of the attention mechanism is the concatenation of the output of each head. The output of the attention mechanism is then processed by a linear layer to produce the output embeddings.</p>
<p>Similarly to the positional encoding, the MHSA is implemented as a PyTorch module. The input and output of the MHSA are embeddings, therefore their shape does not change $<span class="math notranslate nohighlight">\((B, S, E) \rightarrow (B, S, E)\)</span>$.</p>
<div class="tip admonition">
<p class="admonition-title">Test MHSA implementation</p>
<p>We can test the implementation of the MHSA module by creating a random input tensor and passing it through the module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">mhsa</span> <span class="kn">import</span> <span class="n">MHSA</span> <span class="c1"># import the MHSA module</span>

<span class="n">mhsa</span> <span class="o">=</span> <span class="n">MHSA</span><span class="p">(</span><span class="n">embedding_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span> <span class="c1"># create the MHSA module</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># create a random input tensor</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mhsa</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># pass the input tensor through the MHSA module</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># print the shape of the output tensor</span>
</pre></div>
</div>
</div>
</section>
<section id="cross-attention">
<h3>Cross-Attention<a class="headerlink" href="#cross-attention" title="Link to this heading">#</a></h3>
<p>The cross-attention layer is similar to the self-attention layer of the encoder. The only difference is that the cross-attention layer is used to learn the relationships between the elements of the input sequence and the elements of the output sequence.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">CrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">=</span> <span class="n">embedding_size</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        x: input embeddings</span>
<span class="sd">        y: vector to &quot;cross-attend&quot; to</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">E</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># queries are computed from x</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># keys are computed from y</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># values are computed from y</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">E</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
<p>As noted also in code comments, the queries are computed from the input embeddings <span class="math notranslate nohighlight">\(x\)</span> while the keys and values are computed from the vector <span class="math notranslate nohighlight">\(y\)</span>. All the other considerations are the same as the self-attention layer (e.g., the output of the cross-attention layer is the concatenation of the output of each head).</p>
</section>
<section id="id21">
<h3>Feed-Forward and Residual Connections<a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
<p>After the attention mechanism, the output embeddings are processed by a feed-forward network. The feed-forward network is a simple linear layer followed by a non-linear activation function (e.g., ReLU).</p>
<p>Similarly to what we have seen with ResNets <span id="id22">[]</span>, in each layer of the encoder and decoder, there are <em>residual connections</em> that sum up the output of a sub-layer with the input of the sub-layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">Residual</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sublayer</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">sublayer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">sublayer</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</pre></div>
</div>
<p>The class <code class="docutils literal notranslate"><span class="pre">FeedForward</span></code> implements the feed-forward network. The class <code class="docutils literal notranslate"><span class="pre">Residual</span></code> implements the residual connections. The Residual class is usually not used in real implementations of transformer models.
Instead, the residual connections are implemented directly in the encoder and decoder layers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ... other code ...</span>
<span class="n">x</span> <span class="c1"># is the tensor we want to add for the residual connection</span>
<span class="n">x_ff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># pass x through the feed-forward network</span>
<span class="n">x_ff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x_ff</span><span class="p">)</span> <span class="c1"># apply dropout</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x_ff</span> <span class="c1"># add the residual connection</span>
</pre></div>
</div>
</section>
<section id="id23">
<h3>Encoder and Decoder Models<a class="headerlink" href="#id23" title="Link to this heading">#</a></h3>
<p>At this point we have all the ingredients to create an encoder transformer layer. The encoder model is composed of:</p>
<ul class="simple">
<li><p><strong>Embedding layer</strong>. The embedding layer converts the input sequence into a sequence of vector embeddings. If we deal with vectorized data (e.g., images), we can skip this step or implement it differently <span id="id24">[<a class="reference internal" href="references.html#id21" title="Andy T Liu, Shu-wen Yang, Po-Han Chi, Po-chun Hsu, and Hung-yi Lee. Mockingjay: unsupervised speech representation learning with deep bidirectional transformer encoders. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 6419–6423. IEEE, 2020.">LYC+20</a>]</span>.</p></li>
<li><p><strong>Positional encoding</strong>. The positional encoding injects information about the position of each element of the sequence into the model.</p></li>
<li><p><strong>Encoder layers</strong>. The encoder layers process the embeddings using:</p>
<ul>
<li><p><strong>Multi-head attention</strong>. The multi-head attention mechanism is used to learn the relationships between the different elements of the sequence.</p></li>
<li><p><strong>Feed-forward network</strong>. The feed-forward network is used to process the output of the multi-head attention mechanism.</p></li>
<li><p><strong>Residual connections</strong>. The residual connections are used to sum up the output of the multi-head attention mechanism with the input embeddings.</p></li>
<li><p><strong>Layer normalization</strong>. The layer normalization is used to normalize the output of the encoder layer.</p></li>
</ul>
</li>
<li><p><strong>Output</strong>. The output of the encoder is the output embeddings of the last encoder layer.</p></li>
</ul>
<p>A stack of encoder layers is used to create the encoder model. The output of the encoder is the output embeddings of the last encoder layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mhsa</span> <span class="o">=</span> <span class="n">MHSA</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">residual1</span> <span class="o">=</span> <span class="n">Residual</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mhsa</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">residual2</span> <span class="o">=</span> <span class="n">Residual</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>The class <code class="docutils literal notranslate"><span class="pre">EncoderLayer</span></code> implements a single encoder layer. The class <code class="docutils literal notranslate"><span class="pre">Encoder</span></code> implements a stack of encoder layers. The output of the encoder is the output embeddings of the last encoder layer.</p>
<div class="tip admonition">
<p class="admonition-title">Note on the number of layers</p>
<p>The number of layers is a <strong>hyperparameter</strong> of the model. The number of layers is usually between 6 and 12. The number of layers is usually the same for the encoder and decoder. However, it is possible to use a different number of layers for the encoder and decoder.</p>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h1>
<p>In this chapter, we have seen all the components behind one of the most popular deep learning architectures: the transformer (encoder decoder) and its encoder-only and decoder-only variants. We have seen how to implement the different components of the transformer architecture in pure PyTorch.</p>
<p>This architecture has basically revolutionized the field of deep learning. It has been used in many different domains (e.g., NLP, audio, images, multi-modal data, etc.) and has achieved state-of-the-art results in many different tasks (e.g., speech recognition, machine translation, image classification, etc.).</p>
<p>In the next chapter, we will get our hands dirty and we will see how to use the transformer architecture in practice, both starting from scratch and using pre-trained models.</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="2_cnns.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Convolutional Neural Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="4_libraries.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Deep Learning Libraries</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Transformers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-overview">Architecture Overview</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-decoder">Encoder-Decoder</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-components">Transformer Components</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-layer">Embedding Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanism">Attention Mechanism</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-and-residual-connections">Feed-Forward and Residual Connections</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-and-decoder-models">Encoder and Decoder Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder-decoder-sequence-to-sequence-models">Encoder-Decoder (Sequence-to-sequence) Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-encoder-decoder-model-in-pytorch">An Encoder-Decoder Model in PyTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Embedding Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Positional Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Attention Mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention">Cross-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Feed-Forward and Residual Connections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">Encoder and Decoder Models</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Moreno La Quatra
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>