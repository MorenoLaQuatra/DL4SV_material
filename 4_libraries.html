
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Deep Learning Libraries &#8212; Deep Learning for Speech and Vision</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js?v=afe5de03"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '4_libraries';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Applications and final projects" href="5_applications.html" />
    <link rel="prev" title="Transformers" href="3_transformers.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/cover_2.png" class="logo__image only-light" alt="Deep Learning for Speech and Vision - Home"/>
    <script>document.write(`<img src="_static/cover_2.png" class="logo__image only-dark" alt="Deep Learning for Speech and Vision - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Deep Learning for Speech and Vision
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_dl_intro.html">Introduction to Deep Learning</a></li>


<li class="toctree-l1"><a class="reference internal" href="2_cnns.html">Convolutional Neural Networks</a></li>




<li class="toctree-l1"><a class="reference internal" href="3_transformers.html">Transformers</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#">Deep Learning Libraries</a></li>


<li class="toctree-l1"><a class="reference internal" href="5_applications.html">Applications and final projects</a></li>



<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F4_libraries.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/4_libraries.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deep Learning Libraries</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Deep Learning Libraries</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Deep learning libraries</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">PyTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors">Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autograd">Autograd</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizers-and-schedulers">Optimizers and Schedulers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets-and-dataloaders">Datasets and DataLoaders</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-support">GPU support</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-training">Distributed training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#huggingface-transformers">HuggingFace Transformers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-monitoring-and-visualization">Model monitoring and visualization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comet">Comet</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deep-learning-libraries">
<h1>Deep Learning Libraries<a class="headerlink" href="#deep-learning-libraries" title="Link to this heading">#</a></h1>
<figure class="align-default" id="cover">
<a class="reference internal image-reference" href="_images/cover_app.png"><img alt="cover" src="_images/cover_app.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 46 </span><span class="caption-text">Image generated using <a class="reference external" href="https://huggingface.co/spaces/mrfakename/OpenDalleV1.1-GPU-Demo">OpenDALL-E</a></span><a class="headerlink" href="#cover" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In this chapter, we will see some some libraries that are useful to build, train and monitor deep learning models. More specifically, we will see:</p>
<ul class="simple">
<li><p><strong>PyTorch</strong>: a Python library that provides a wide range of tools to build and train deep learning models.</p></li>
<li><p><strong>HuggingFace Transformers</strong>: Python library that provides access to pre-trained models for a variety of domains, including NLP, Computer Vision and Speech Processing.</p></li>
<li><p><strong>Comet</strong>: an online platform that allows you to monitor specific metrics during training.</p></li>
</ul>
<div class="learning-objectives admonition">
<p class="admonition-title">Learning Objectives</p>
<ul class="simple">
<li><p>Understand the main concepts of PyTorch</p></li>
<li><p>Analyze the main features of HuggingFace Transformers and how to use them</p></li>
<li><p>How to monitor the training process to identify potential issues and understand the model behavior</p></li>
</ul>
</div>
<p>During the exercises, you will be asked to implement specific components of common deep learning pipelines. You will be asked to use the libraries presented in this chapter. However, you are encouraged to use other libraries if you prefer.
For the exercises, you can use the <a class="reference external" href="https://colab.research.google.com/">Google Colab</a> environment. It is a free environment that allows you to run Python code in the cloud. It also provides free GPUs and TPUs. You can find more information <a class="reference external" href="https://colab.research.google.com/notebooks/intro.ipynb">here</a>.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>Deep learning libraries<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<p>There are many deep learning libraries available. Some of them are more general, while others are more specific. In this section, we will see some of the most popular libraries.</p>
<section id="pytorch">
<h2>PyTorch<a class="headerlink" href="#pytorch" title="Link to this heading">#</a></h2>
<p>From a research perspective, PyTorch is one of the most popular deep learning libraries. It is a Python library that provides a wide range of tools to build and train deep learning models. It is also the library used in this book. PyTorch is based on the Torch library, which is a scientific computing framework with wide support for machine learning algorithms. PyTorch is developed by Facebook’s AI Research lab (FAIR) and since September 2022, <a class="reference external" href="https://www.linuxfoundation.org/blog/blog/welcoming-pytorch-to-the-linux-foundation">it is a Linux Foundation project</a>.</p>
<p>PyTorch is a Python library that allows you to build and train deep learning models. Some of the core concepts of PyTorch are:</p>
<ul class="simple">
<li><p><strong>Tensors</strong>: Tensors are the core data structure of PyTorch. They are similar to NumPy arrays, but they can be used on GPUs to accelerate computing.</p></li>
<li><p><strong>Autograd</strong>: PyTorch provides automatic differentiation for all operations on Tensors. This means that you can compute gradients automatically. Namely, you can compute the gradients of the loss function with respect to the parameters of the model.</p></li>
<li><p><strong>Neural networks</strong>: PyTorch provides a wide range of neural network layers and activation functions. It also provides a way to build neural networks using the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class (similarly to what we have seen in the previous chapters).</p></li>
<li><p><strong>Optimizers</strong>: PyTorch provides a wide range of optimizers, such as SGD, Adam, RMSProp, etc.</p></li>
<li><p><strong>Data loaders</strong>: PyTorch provides a way to load data in batches. This is useful when you have a large dataset and you want to train your model on a GPU.</p></li>
<li><p><strong>GPU support</strong>: PyTorch provides GPU support for all operations on Tensors. This means that you can train your model on a GPU and get faster results.</p></li>
<li><p><strong>Distributed training</strong>: PyTorch provides a way to train your model on multiple GPUs or multiple machines. This is useful when you have a large dataset and you want to train your model on multiple GPUs or multiple machines.</p></li>
</ul>
<section id="tensors">
<h3>Tensors<a class="headerlink" href="#tensors" title="Link to this heading">#</a></h3>
<p>A tensor is a generalization of vectors and matrices. A tensor is a multidimensional array. For example, a vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, and a 3-dimensional tensor is a 3-dimensional array. In PyTorch, tensors are represented by the <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> class. The <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> class is similar to the NumPy <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> class. However, the <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> class provides GPU support for all operations on tensors. This means that you can perform operations on tensors on a GPU and get faster results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Create a tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="c1"># shape: (2, 3)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># dtype: torch.int64</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="c1"># device: cpu</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Create a tensor on GPU</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span> <span class="c1"># you can use mps on modern MacBooks</span>
<span class="c1"># ...</span>
</pre></div>
</div>
</section>
<section id="autograd">
<h3>Autograd<a class="headerlink" href="#autograd" title="Link to this heading">#</a></h3>
<p>When you train a deep learning model, you need to compute the gradients of the loss function with respect to the parameters of the model. This is called backpropagation. PyTorch provides automatic differentiation for all operations on tensors. What this means is that you can compute gradients automatically. For example, if you have a tensor <code class="docutils literal notranslate"><span class="pre">x</span></code> and you want to compute the gradients of the loss function with respect to <code class="docutils literal notranslate"><span class="pre">x</span></code>, you can do it with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># only complex or float tensors can have gradients</span>
<span class="c1"># requires_grad=True means that we want to compute gradients</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># compute gradients</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># gradients of z with respect to x</span>
</pre></div>
</div>
<p>This is useful when you want to train a deep learning model. The gradients are computed automatically, so you don’t have to backpropagate them manually. Autograd works by keeping track of all operations on tensors. When you call <code class="docutils literal notranslate"><span class="pre">backward()</span></code> on a tensor, autograd computes the gradients of the loss function with respect to the tensor. This is done by using the chain rule.</p>
</section>
<section id="neural-networks">
<h3>Neural networks<a class="headerlink" href="#neural-networks" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> module provides a wide range of neural network layers and activation functions. Convolutional, Transformer, Pooling… layers are available.
One of the most important classes in the <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> module is the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class. This class is used to build neural networks. It provides a way to define the forward pass of the neural network. The forward pass is the process of computing the output of the neural network given an input. The <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class also provides a way to define the backward pass of the neural network. The backward pass is the process of computing the gradients of the loss function with respect to the parameters of the neural network. In most cases, you don’t need to define the backward pass manually so you can inherit from the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class and define the forward pass only.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="optimizers-and-schedulers">
<h3>Optimizers and Schedulers<a class="headerlink" href="#optimizers-and-schedulers" title="Link to this heading">#</a></h3>
<p>The optimizer of a neural network is the algorithm that is used to update the parameters of the neural network. The most popular optimizer is the Stochastic Gradient Descent (SGD) optimizer. It is used to update the parameters of the neural network by computing the gradients of the loss function with respect to the parameters of the neural network and then updating the parameters of the neural network using the gradients. PyTorch provides a wide range of optimizers, such as SGD, Adam, AdamW, RMSProp, etc.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
<p>The optimizer takes as input the parameters of the neural network and the learning rate. The learning rate is a hyperparameter that controls how much the parameters of the neural network are updated.
Most of the times, however, you don’t want a constant learning rate. You want to <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">adjust the learning rate during training</a>. This is done by using a learning rate scheduler. PyTorch provides a wide range of learning rate schedulers, such as StepLR, MultiStepLR, ExponentialLR, etc.
For example, if we want to decrease the learning rate by a factor of 0.1 every 10 epochs, we can do it with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>💡 When using optimizers and schedulers, it is important to include them in the training loop. Each object has a <code class="docutils literal notranslate"><span class="pre">step()</span></code> method that should be called after each batch. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="c1"># ...</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<p>❓ Why do we need to call <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code> after each batch?
When we compute the gradients using <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>, the gradients are accumulated in the parameters of the neural network. Those gradients are used in <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> to update the parameters of the neural network.
Next time we call <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>, the gradients are accumulated in the parameters of the neural network again. This means that the gradients are accumulated over and over at each batch. This is not what we want. Instead, we want to compute the gradients for each batch and then update the parameters of the neural network. After that, we want to reset the gradients to zero so that the gradients are not accumulated over and over at each batch. This is done by calling <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
</section>
<section id="datasets-and-dataloaders">
<h3>Datasets and DataLoaders<a class="headerlink" href="#datasets-and-dataloaders" title="Link to this heading">#</a></h3>
<p>When training a deep learning model, you need to load the data to feed it to the model. In almost all cases, you don’t want to feed a single example to the model. Instead, you want to use a batch of examples. This is done by using data loaders. <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> are two classes that allow data manipulation in PyTorch.</p>
<p>Detailed explainations of <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> are available in the <a class="reference internal" href="2_cnns.html"><span class="std std-doc">CNN chapter</span></a>. Hereafter we provide a brief summary of the two classes.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class is used to load the data from the dataset. It is designed to provide a way to load the data from the dataset and to transform the data. The <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> class is used to load the data from the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class. It provides a unified interface to divide the data into batches and to load the data from the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class.</p>
<div class="exercise admonition">
<p class="admonition-title">Exercise, 30 min</p>
<p>Select a dataset that is useful for your research. If you don’t have a dataset, you can use the <a class="reference external" href="https://www.openslr.org/110/">German Emotional-TTS dataset</a> that contains 300 identical sentences spoken by a single speaker in 8 different emotions (2.400 recordings in total for 175 minutes of speech).</p>
<p>👀 <strong>Hint</strong>: You can download the folder by executing the following command in your terminal: <code class="docutils literal notranslate"><span class="pre">wget</span> <span class="pre">https://www.openslr.org/resources/110/thorsten-emotional_v02.tgz</span></code></p>
<p>Create a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class that loads the data from the dataset. Then, create a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> class that loads the data from the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class.</p>
</div>
</section>
<section id="gpu-support">
<h3>GPU support<a class="headerlink" href="#gpu-support" title="Link to this heading">#</a></h3>
<p>PyTorch provides GPU support for all operations on tensors. This means that you can perform operations on tensors on a GPU and get faster results. To use a GPU, you need to create a tensor on a GPU. This is done by using the <code class="docutils literal notranslate"><span class="pre">device</span></code> argument of the <code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code> function. For example, if you want to create a tensor on a GPU, you can do it with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>To benchmark the speed of a GPU we can run the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="n">gpu_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;10 x GEMM on GPU: </span><span class="si">{</span><span class="n">gpu_time</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">cpu_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;10 x GEMM on CPU: </span><span class="si">{</span><span class="n">cpu_time</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU speedup: </span><span class="si">{</span><span class="n">cpu_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">gpu_time</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># GEMM stands for General Matrix Multiplication</span>
</pre></div>
</div>
<p>PyTorch does not support only NVIDIA GPUs. It also supports modern MacBooks with Apple Silicon. You can use the <code class="docutils literal notranslate"><span class="pre">device='mps'</span></code> argument to use the Apple Silicon GPU.</p>
</section>
<section id="distributed-training">
<h3>Distributed training<a class="headerlink" href="#distributed-training" title="Link to this heading">#</a></h3>
<p>When dealing with large models and large datasets, it is often necessary to train the model on multiple GPUs. PyTorch provides a way to train your model on multiple GPUs. This is done by using the <code class="docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code> class. This class allows you to train your model on multiple GPUs by splitting the batch into multiple batches and then computing the gradients on each GPU. For example, if you have a batch of size 64 and you want to train your model on 2 GPUs, you can do it with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># ... define model ...</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># replicate model to multiple GPUs</span>

<span class="c1"># ... create data loader ...</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
    <span class="c1"># device management will be done automatically</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># send input to GPU</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="c1"># ... compute loss ...</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># save model</span>
<span class="c1"># without DataParallel</span>
<span class="c1"># torch.save(model.state_dict(), &#39;model.pt&#39;)</span>
<span class="c1"># with DataParallel</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;model.pt&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>PyTorch also provides a way to train your model on multiple machines (DDP). This is done by using the <code class="docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel</span></code> class. This class allows you to train your model on a single machine with multiple GPUs or multiple machines with multiple GPUs. However, it requires more setup than the <code class="docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code> class. For example, if you want to train your model on 2 machines with 2 GPUs each, you can do it with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># ... define model ...</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">(),</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">model</span><span class="p">,))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">train</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">gpu</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">gpu</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu</span><span class="p">)</span>
    <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">gpu</span><span class="p">])</span>
    <span class="c1"># ... create data loader ...</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">ddp_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="c1"># ... compute loss ...</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>The setup is more complicated than the <code class="docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code> class because you need to initialize the process group and set the device manually. This process is designed for more advanced use-cases. If you are interested in distributed training, you may want to take a look at <a class="reference external" href="https://www.pytorchlightning.ai/">PyTorch Lightning</a>, a library that both simplifies and extends PyTorch’s capabilities. PyTorch Lightning is beyond the scope of this book, and you can contact the teacher if you want to learn more about it.</p>
</section>
</section>
<section id="huggingface-transformers">
<h2>HuggingFace Transformers<a class="headerlink" href="#huggingface-transformers" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://huggingface.co/transformers/">HuggingFace Transformers</a> is a Python library that provides state-of-the-art models for a variety of domains, including NLP, Computer Vision and Speech Processing. As can be inferred from the name, the library mostly include transformer-based models.</p>
<p>The library is built on top of PyTorch (with some models also implemented in TensorFlow and JAX) and provides a unified API to use the models. The library allows the use of pre-trained models that can be found on the <a class="reference external" href="https://huggingface.co/models">Model Hub</a>.</p>
<figure class="align-default" id="hf-hub">
<a class="reference internal image-reference" href="_images/huggingface.png"><img alt="hf_hub" src="_images/huggingface.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 47 </span><span class="caption-text">Screenshot of the HuggingFace Model Hub</span><a class="headerlink" href="#hf-hub" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>As of January 2024, the hub includes almost 500.000 models (both pre-trained and fine-tuned) and is growing fast. It is possible to <strong>load</strong> pre-trained models, <strong>train</strong> new models, <strong>evaluate</strong> models and <strong>share</strong> models.</p>
<p>Depending on the model and the task, the library provides different classes and methods. However, the general workflow is the same for all models. The following code shows how to load a pre-trained model and use it to generate text:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Wav2Vec2ForSequenceClassification</span><span class="p">,</span> <span class="n">AutoFeatureExtractor</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;facebook/wav2vec2-base&quot;</span> <span class="c1"># identifier of the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Wav2Vec2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span> <span class="c1"># load the model</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">AutoFeatureExtractor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span> <span class="c1"># load the feature extractor</span>

<span class="n">sample_audio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16000</span><span class="p">)</span> <span class="c1"># 1 second of audio sampled at 16kHz</span>
<span class="n">input_values</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="n">sample_audio</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_values</span> <span class="c1"># extract features</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_values</span><span class="p">)</span> <span class="c1"># forward pass</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span> <span class="c1"># get logits</span>
</pre></div>
</div>
<p>Most of the parameters are initialized by <strong>default</strong>, so it is suggested to carefully read the documentation of the model you want to use. For example, in the previous code, the classification <strong>head</strong> is initialized with 2 classes (positive and negative). If you want to use a different number of classes, you need to change the <code class="docutils literal notranslate"><span class="pre">num_labels</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Wav2Vec2ForSequenceClassification</span><span class="p">,</span> <span class="n">AutoFeatureExtractor</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;facebook/wav2vec2-base&quot;</span> <span class="c1"># identifier of the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Wav2Vec2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># load the model</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">AutoFeatureExtractor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span> <span class="c1"># load the feature extractor</span>

<span class="n">sample_audio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16000</span><span class="p">)</span> <span class="c1"># 1 second of audio sampled at 16kHz</span>
<span class="n">input_values</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="n">sample_audio</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_values</span> <span class="c1"># extract features</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_values</span><span class="p">)</span> <span class="c1"># forward pass</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span> <span class="c1"># get logits (shape: (1, 3))</span>
</pre></div>
</div>
<p>Pre-trained models, in most cases, can be used as <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> objects. This means that you can use them in the same way you use PyTorch models. For example, you can train them using the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class and the <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Wav2Vec2ForSequenceClassification</span><span class="p">,</span> <span class="n">AutoFeatureExtractor</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;facebook/wav2vec2-base&quot;</span> <span class="c1"># identifier of the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Wav2Vec2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># load the model</span>

<span class="c1"># ... create data loader ...</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
    <span class="n">input_values</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;input_values&#39;</span><span class="p">]</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_values</span><span class="p">)</span> <span class="c1"># forward pass</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span> <span class="c1"># get loss</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># compute gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># update parameters</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># reset gradients</span>
</pre></div>
</div>
<p>It is <strong>suggested</strong> to extract features in the <code class="docutils literal notranslate"><span class="pre">__get_item__</span></code> method of the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class, or in the <code class="docutils literal notranslate"><span class="pre">data_collator</span></code> of the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> class. This is because the feature extraction is usually computationally <em>expensive</em>, thus parallelizing it is a good idea.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchaudio</span>

<span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">paths</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paths</span> <span class="o">=</span> <span class="n">paths</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">AutoFeatureExtractor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">audio</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">torchaudio</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="c1"># ... resample, normalize, etc ...</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_values</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;input_values&quot;</span><span class="p">:</span> <span class="n">features</span><span class="p">,</span>
            <span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">label</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>The library also provides a <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class that automate most of the training process. However, the simplicity comes at the cost of flexibility, thus to do non-standard training, it is suggested to implement the training loop manually. Using the trainer class is beyond the scope of this book, but you can find more information <a class="reference external" href="https://huggingface.co/transformers/main_classes/trainer.html">here</a>.</p>
<div class="exercise admonition">
<p class="admonition-title">Exercise, 40 min</p>
<p>Select a model from the <a class="reference external" href="https://huggingface.co/models">Model Hub</a> that is useful for your research. Use the Dataset you created in the previous exercise to fine-tune the model (e.g., for classification).</p>
<p>Implement a complete training setup, inclusing training, validation and testing. You can set standard parameters (e.g., batch size, learning rate, etc.) for educational purposes. However, you are encouraged to experiment with different parameters and to use a validation set to select the best model.</p>
</div>
</section>
<section id="model-monitoring-and-visualization">
<h2>Model monitoring and visualization<a class="headerlink" href="#model-monitoring-and-visualization" title="Link to this heading">#</a></h2>
<p>When training a deep learning model, it is important to monitor the training process. There exist many tools to monitor the training process:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.tensorflow.org/tensorboard">TensorBoard</a></p></li>
<li><p><a class="reference external" href="https://wandb.ai/site">Weights &amp; Biases</a></p></li>
<li><p><a class="reference external" href="https://www.comet.ml/site/">Comet</a></p></li>
<li><p><a class="reference external" href="https://neptune.ai/">Neptune</a></p></li>
<li><p>… and many more</p></li>
</ul>
<p>In this section, we will see how to use Comet to monitor the training process. It is an online platform that allows you to monitor specific metrics during training.</p>
<section id="comet">
<h3>Comet<a class="headerlink" href="#comet" title="Link to this heading">#</a></h3>
<p>To use Comet, you need to create an account on the <a class="reference external" href="https://www.comet.ml/site/">website</a>. <strong>Note</strong>: Comet is free for academic use. Once you have created an account, you can install the Python library by executing the following command in your terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>comet-ml
</pre></div>
</div>
<p>Then, you can use the library to log metrics during training. For example, if you want to log the loss during training, you can do it with the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">comet_ml</span> <span class="kn">import</span> <span class="n">Experiment</span>

<span class="c1"># ... create model ...</span>
<span class="n">experiment</span> <span class="o">=</span> <span class="n">Experiment</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;your_api_key&quot;</span><span class="p">,</span>
    <span class="n">workspace</span><span class="o">=</span><span class="s2">&quot;your_workspace&quot;</span><span class="p">,</span> 
    <span class="n">project_name</span><span class="o">=</span><span class="s2">&quot;your_project_name&quot;</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="c1"># ... forward pass ...</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
        <span class="n">experiment</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>Hereafter we explain each parameter of the <code class="docutils literal notranslate"><span class="pre">Experiment</span></code> class:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">api_key</span></code>: This is the API key that you can find on the <a class="reference external" href="https://www.comet.ml/site/">website</a>. You can also find it in the settings of your account.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">workspace</span></code>: This is the name of your workspace. Usually, it is your username.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">project_name</span></code>: This is the name of your project. If the project does not exist, it will be created automatically (e.g., <code class="docutils literal notranslate"><span class="pre">project_name=&quot;DL4SV-course&quot;</span></code>).</p></li>
</ul>
<p>⚠️ It is strongly suggested to <strong>not</strong> hard-code the API key in your code. Instead, you can use environment variables. For example, you can use the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">comet_ml</span> <span class="kn">import</span> <span class="n">Experiment</span>

<span class="n">experiment</span> <span class="o">=</span> <span class="n">Experiment</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;COMET_API_KEY&quot;</span><span class="p">],</span>
    <span class="n">workspace</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;COMET_WORKSPACE&quot;</span><span class="p">],</span> 
    <span class="n">project_name</span><span class="o">=</span><span class="s2">&quot;DL4SV-course&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>To set the environment variables, you can use the following commands in your terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">COMET_API_KEY</span><span class="o">=</span><span class="s2">&quot;your_api_key&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">COMET_WORKSPACE</span><span class="o">=</span><span class="s2">&quot;your_workspace&quot;</span>
</pre></div>
</div>
<p>or you can add them to your <code class="docutils literal notranslate"><span class="pre">~/.bashrc</span></code> file to be loaded automatically at the lauch of a new terminal.</p>
<p>Once your training/evaluation is running, you can go to the <a class="reference external" href="https://www.comet.ml/site/">website</a> and see the metrics evolving in real-time.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/comet.png"><img alt="comet" src="_images/comet.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 48 </span><span class="caption-text">Example of Comet dashboard from the <a class="reference external" href="https://www.comet.ml/site/">website</a></span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>There are additional and more advanced features that you can use. For example, you can log images, audio, text, etc. You can also log hyperparameters and tags. You can find more information <a class="reference external" href="https://www.comet.ml/docs/python-sdk/Experiment/">here</a>.</p>
<div class="exercise admonition">
<p class="admonition-title">Exercise, 15 min</p>
<p>Modify the code of the previous exercise to log the loss during training. You can also log the validation loss and the accuracy.</p>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h1>
<p>In this chapter, we have seen some of the most popular deep learning libraries. We have seen:</p>
<ul class="simple">
<li><p>Libraries to build and train deep learning models - PyTorch in particular. It is the <strong>de-facto</strong> standard for research in computer vision, speech processing and many other domains.</p></li>
<li><p>Libraries to use pre-trained models - HuggingFace Transformers in particular. It allows to use pre-trained models in a simple and unified way. It focuses on transformer-based models, but it also includes other models.</p></li>
<li><p>Libraries to monitor the training process - Comet in particular. It allows to monitor specific metrics during training. It is free for academic use.</p></li>
</ul>
<p>While this list is <strong>by no means exhaustive</strong>, it should give you a good starting point to create, manage and monitor your projects in the deep learning field. If you have specific requirements or you want to use a different library, you are encouraged to do so it by yourself or contact the teacher for more information.</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="3_transformers.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Transformers</p>
      </div>
    </a>
    <a class="right-next"
       href="5_applications.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Applications and final projects</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Deep Learning Libraries</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Deep learning libraries</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch">PyTorch</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors">Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autograd">Autograd</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizers-and-schedulers">Optimizers and Schedulers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets-and-dataloaders">Datasets and DataLoaders</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-support">GPU support</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-training">Distributed training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#huggingface-transformers">HuggingFace Transformers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-monitoring-and-visualization">Model monitoring and visualization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comet">Comet</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Moreno La Quatra
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>